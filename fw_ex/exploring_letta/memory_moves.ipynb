{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cab81da-819e-4342-8dbb-d05fa12c34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta import create_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d53d67-200c-4c80-937e-56b912d76f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d125bd61-2701-43eb-8a74-39e4bf0ec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = client.create_block(label=\"persona\", text=\"Lets keep making\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0da9750-7507-4205-ac9d-a5d147768a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making memories\n",
    "from letta.schemas.memory import BasicBlockMemory\n",
    "\n",
    "m1 = BasicBlockMemory(blocks=[b1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc39de49-e988-419b-9b2b-62fb6841ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = BasicBlockMemory(blocks=[b1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936d3e87-d512-4caf-bd56-91eb9a1ec3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = client.list_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f2ce15-cff6-4b46-8ba7-ad86199e0c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'groqagent'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "919183d1-2bf2-44c5-b62c-258a54e56b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Memory(memory={'human': Block(value='First name: kay\\n', limit=2000, template_name='basic', template=False, label='human', description=None, metadata_={}, user_id=None, id='block-716b3b9a-ed78-4eba-a466-5a8cccebd330'), 'persona': Block(value='I am an expert reasoning agent that can do the following:\\nMy name is Letta.\\nI am kind, thoughtful, and inquisitive.', limit=2000, template_name='o1_persona', template=False, label='persona', description=None, metadata_={}, user_id=None, id='block-e70e9767-aaa6-4e69-9ce6-3b60e9d41938')}, prompt_template='{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmem = client.get_core_memory(agents[1].id)\n",
    "cmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ec4856-0ed2-4ba8-b322-cef199c98793",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentState(description=None, metadata_={'human:': 'basic', 'persona': 'sam_pov'}, user_id='user-00000000-0000-4000-8000-000000000000', id='agent-9dd1f209-170a-4067-9293-c6683eb39456', name='Ruby', created_at=datetime.datetime(2024, 11, 12, 22, 4, 15, 632842), message_ids=['message-bf8cd676-7b51-4bd5-9d41-fb326197bf0a', 'message-1857cf3c-3c2e-4aed-99fc-b9887ae18185', 'message-effe30d5-e4ca-44b6-a67a-c00418bd085a', 'message-d0962483-8068-44f7-ae72-787b1da5e2e0', 'message-673afbd1-e001-4753-95aa-61bbd6838958', 'message-7d60be12-849f-4cbd-99a7-ee21b3dc5466', 'message-2bb12fb2-2615-4d94-a8a4-dc9a83662b50', 'message-d19dead5-de82-4efc-beb9-e1846748f15b', 'message-a355e5c2-55bc-4640-bce1-40143101be80', 'message-caa39a86-7d1c-46f1-98f8-be31dd274f8d'], memory=Memory(memory={'persona': Block(value='Understand the requirement from the user and provide assstance and support', limit=2000, template_name=None, template=False, label='persona', description=None, metadata_={}, user_id=None, id='block-31f53a3f-6cba-4fc4-8459-2a7a26404f45'), 'human': Block(value='My name is Ruby', limit=2000, template_name=None, template=False, label='human', description=None, metadata_={}, user_id=None, id='block-fa90d31f-6ee3-40e5-b5e3-e7d1f5aa2097')}, prompt_template='{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}'), tools=['send_message', 'conversation_search', 'conversation_search_date', 'archival_memory_insert', 'archival_memory_search', 'core_memory_append', 'core_memory_replace'], tool_rules=[TerminalToolRule(tool_name='send_message', type='TerminalToolRule')], tags=[], system='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.', agent_type=<AgentType.memgpt_agent: 'memgpt_agent'>, llm_config=LLMConfig(model='gemma2-9b-it', model_endpoint_type='groq', model_endpoint='https://api.groq.com/openai/v1', model_wrapper=None, context_window=8192, put_inner_thoughts_in_kwargs=True), embedding_config=EmbeddingConfig(embedding_endpoint_type='hugging-face', embedding_endpoint='https://embeddings.memgpt.ai', embedding_model='letta-free', embedding_dim=1024, embedding_chunk_size=300, azure_endpoint=None, azure_version=None, azure_deployment=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rubyid = \"agent-9dd1f209-170a-4067-9293-c6683eb39456\"\n",
    "ruby_agent = client.get_agent_by_name(agent_name=\"Ruby\")\n",
    "ruby_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437cbc96-c562-422d-b668-3b86f5caaf5b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': None,\n",
       " 'metadata_': {'human:': 'basic', 'persona': 'sam_pov'},\n",
       " 'user_id': 'user-00000000-0000-4000-8000-000000000000',\n",
       " 'id': 'agent-9dd1f209-170a-4067-9293-c6683eb39456',\n",
       " 'name': 'Ruby',\n",
       " 'created_at': datetime.datetime(2024, 11, 12, 22, 4, 15, 632842),\n",
       " 'message_ids': ['message-bf8cd676-7b51-4bd5-9d41-fb326197bf0a',\n",
       "  'message-1857cf3c-3c2e-4aed-99fc-b9887ae18185',\n",
       "  'message-effe30d5-e4ca-44b6-a67a-c00418bd085a',\n",
       "  'message-d0962483-8068-44f7-ae72-787b1da5e2e0',\n",
       "  'message-673afbd1-e001-4753-95aa-61bbd6838958',\n",
       "  'message-7d60be12-849f-4cbd-99a7-ee21b3dc5466',\n",
       "  'message-2bb12fb2-2615-4d94-a8a4-dc9a83662b50',\n",
       "  'message-d19dead5-de82-4efc-beb9-e1846748f15b',\n",
       "  'message-a355e5c2-55bc-4640-bce1-40143101be80',\n",
       "  'message-caa39a86-7d1c-46f1-98f8-be31dd274f8d'],\n",
       " 'memory': {'memory': {'persona': {'value': 'Understand the requirement from the user and provide assstance and support',\n",
       "    'limit': 2000,\n",
       "    'template_name': None,\n",
       "    'template': False,\n",
       "    'label': 'persona',\n",
       "    'description': None,\n",
       "    'metadata_': {},\n",
       "    'user_id': None,\n",
       "    'id': 'block-31f53a3f-6cba-4fc4-8459-2a7a26404f45'},\n",
       "   'human': {'value': 'My name is Ruby',\n",
       "    'limit': 2000,\n",
       "    'template_name': None,\n",
       "    'template': False,\n",
       "    'label': 'human',\n",
       "    'description': None,\n",
       "    'metadata_': {},\n",
       "    'user_id': None,\n",
       "    'id': 'block-fa90d31f-6ee3-40e5-b5e3-e7d1f5aa2097'}},\n",
       "  'prompt_template': '{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}'},\n",
       " 'tools': ['send_message',\n",
       "  'conversation_search',\n",
       "  'conversation_search_date',\n",
       "  'archival_memory_insert',\n",
       "  'archival_memory_search',\n",
       "  'core_memory_append',\n",
       "  'core_memory_replace'],\n",
       " 'tool_rules': [{'tool_name': 'send_message'}],\n",
       " 'tags': [],\n",
       " 'system': 'You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.',\n",
       " 'agent_type': <AgentType.memgpt_agent: 'memgpt_agent'>,\n",
       " 'llm_config': {'model': 'gemma2-9b-it',\n",
       "  'model_endpoint_type': 'groq',\n",
       "  'model_endpoint': 'https://api.groq.com/openai/v1',\n",
       "  'model_wrapper': None,\n",
       "  'context_window': 8192,\n",
       "  'put_inner_thoughts_in_kwargs': True},\n",
       " 'embedding_config': {'embedding_endpoint_type': 'hugging-face',\n",
       "  'embedding_endpoint': 'https://embeddings.memgpt.ai',\n",
       "  'embedding_model': 'letta-free',\n",
       "  'embedding_dim': 1024,\n",
       "  'embedding_chunk_size': 300,\n",
       "  'azure_endpoint': None,\n",
       "  'azure_version': None,\n",
       "  'azure_deployment': None}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruby_agent.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "590a096b-e4c6-4ffa-8cef-afde3d876005",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:552: UserWarning: API call didn't return a message: {\n",
      "    \"id\": \"chatcmpl-9bd1c437-7bb9-48e6-8243-3f09fa69031d\",\n",
      "    \"choices\": [\n",
      "        null\n",
      "    ],\n",
      "    \"created\": \"2024-11-13T04:57:48Z\",\n",
      "    \"model\": \"gemma2-9b-it\",\n",
      "    \"system_fingerprint\": \"fp_10c08bf97d\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 4,\n",
      "        \"prompt_tokens\": 4840,\n",
      "        \"total_tokens\": 4844\n",
      "    }\n",
      "}\n",
      "  warnings.warn(empty_api_err_message)\n",
      "/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:552: UserWarning: API call didn't return a message: {\n",
      "    \"id\": \"chatcmpl-65413ef2-af5b-4c45-b913-92b06abca011\",\n",
      "    \"choices\": [\n",
      "        null\n",
      "    ],\n",
      "    \"created\": \"2024-11-13T04:57:49Z\",\n",
      "    \"model\": \"gemma2-9b-it\",\n",
      "    \"system_fingerprint\": \"fp_10c08bf97d\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 2,\n",
      "        \"prompt_tokens\": 4840,\n",
      "        \"total_tokens\": 4842\n",
      "    }\n",
      "}\n",
      "  warnings.warn(empty_api_err_message)\n",
      "/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:552: UserWarning: API call didn't return a message: {\n",
      "    \"id\": \"chatcmpl-4b218d34-bf1e-4003-90d0-fc4401ff1540\",\n",
      "    \"choices\": [\n",
      "        null\n",
      "    ],\n",
      "    \"created\": \"2024-11-13T04:57:49Z\",\n",
      "    \"model\": \"gemma2-9b-it\",\n",
      "    \"system_fingerprint\": \"fp_10c08bf97d\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 63,\n",
      "        \"prompt_tokens\": 4840,\n",
      "        \"total_tokens\": 4903\n",
      "    }\n",
      "}\n",
      "  warnings.warn(empty_api_err_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letta.letta.server.server - ERROR - Error in server._step: HTTP error occurred: 429 Client Error: Too Many Requests for url: https://api.groq.com/openai/v1/chat/completions | Status code: 429, Message: {\"error\":{\"message\":\"Rate limit reached for model `gemma2-9b-it` in organization `org_01hrynxzcre36rfxz7t7gh6wdd` on tokens per minute (TPM): Limit 15000, Used 14120, Requested 1747. Please try again in 3.467s. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/helpers.py\", line 48, in make_post_request\n",
      "    response.raise_for_status()\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api.groq.com/openai/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/server/server.py\", line 448, in _step\n",
      "    usage_stats = letta_agent.step(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 825, in step\n",
      "    step_response = self.inner_step(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 1034, in inner_step\n",
      "    raise e\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 950, in inner_step\n",
      "    response = self._get_ai_reply(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 568, in _get_ai_reply\n",
      "    raise e\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 553, in _get_ai_reply\n",
      "    return self._get_ai_reply(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 568, in _get_ai_reply\n",
      "    raise e\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 553, in _get_ai_reply\n",
      "    return self._get_ai_reply(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 568, in _get_ai_reply\n",
      "    raise e\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 553, in _get_ai_reply\n",
      "    return self._get_ai_reply(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 568, in _get_ai_reply\n",
      "    raise e\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py\", line 531, in _get_ai_reply\n",
      "    response = create(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/llm_api_tools.py\", line 66, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/llm_api_tools.py\", line 315, in create\n",
      "    response = openai_chat_completions_request(\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/openai.py\", line 537, in openai_chat_completions_request\n",
      "    response_json = make_post_request(url, headers, data)\n",
      "  File \"/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/helpers.py\", line 75, in make_post_request\n",
      "    raise requests.exceptions.HTTPError(error_message) from http_err\n",
      "requests.exceptions.HTTPError: HTTP error occurred: 429 Client Error: Too Many Requests for url: https://api.groq.com/openai/v1/chat/completions | Status code: 429, Message: {\"error\":{\"message\":\"Rate limit reached for model `gemma2-9b-it` in organization `org_01hrynxzcre36rfxz7t7gh6wdd` on tokens per minute (TPM): Limit 15000, Used 14120, Requested 1747. Please try again in 3.467s. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP error occurred: 429 Client Error: Too Many Requests for url: https://api.groq.com/openai/v1/chat/completions | Status code: 429, Message: {\"error\":{\"message\":\"Rate limit reached for model `gemma2-9b-it` in organization `org_01hrynxzcre36rfxz7t7gh6wdd` on tokens per minute (TPM): Limit 15000, Used 14120, Requested 1747. Please try again in 3.467s. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/helpers.py:48\u001b[0m, in \u001b[0;36mmake_post_request\u001b[0;34m(url, headers, data)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Raise for 4XX/5XX HTTP errors\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Check if the response content type indicates JSON and attempt to parse it\u001b[39;00m\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api.groq.com/openai/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp_ruby \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruby_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[SYSTEM] User has logged back in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/client/client.py:2051\u001b[0m, in \u001b[0;36mLocalClient.send_message\u001b[0;34m(self, message, role, name, agent_id, agent_name, stream_steps, stream_tokens, include_full_message)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m-> 2051\u001b[0m usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMessageCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessageRole\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;66;03m# auto-save\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_save:\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/server/server.py:755\u001b[0m, in \u001b[0;36mSyncServer.send_messages\u001b[0;34m(self, user_id, agent_id, messages, wrap_user_message, wrap_system_message)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll messages must be of type Message or MessageCreate, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mmessage\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmessages]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# Run the agent state forward\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_messages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_objects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/server/server.py:448\u001b[0m, in \u001b[0;36mSyncServer._step\u001b[0;34m(self, user_id, agent_id, input_messages)\u001b[0m\n\u001b[1;32m    445\u001b[0m     token_streaming \u001b[38;5;241m=\u001b[39m letta_agent\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mstreaming_mode \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(letta_agent\u001b[38;5;241m.\u001b[39minterface, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting agent step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m     usage_stats \u001b[38;5;241m=\u001b[39m \u001b[43mletta_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchaining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_chaining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_chaining_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_streaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_verify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    458\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in server._step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:825\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, messages, chaining, max_chaining_steps, ms, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ms\n\u001b[1;32m    824\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_message\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 825\u001b[0m step_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_input_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m step_response\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m    830\u001b[0m heartbeat_request \u001b[38;5;241m=\u001b[39m step_response\u001b[38;5;241m.\u001b[39mheartbeat_request\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:1034\u001b[0m, in \u001b[0;36mAgent.inner_step\u001b[0;34m(self, messages, first_message, first_message_retry_limit, skip_verify, stream, ms)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     printd(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() failed with an unrecognized exception: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:950\u001b[0m, in \u001b[0;36mAgent.inner_step\u001b[0;34m(self, messages, first_message, first_message_retry_limit, skip_verify, stream, ms)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit first message retry limit (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_message_retry_limit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ai_reply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_message_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# Step 3: check if LLM wanted to call a function\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# (if yes) Step 4: call the function\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;66;03m# (if yes) Step 5: send the info on the function call and function response to LLM\u001b[39;00m\n\u001b[1;32m    958\u001b[0m response_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:568\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:553\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;66;03m# Decrement retry limit and try again\u001b[39;00m\n\u001b[1;32m    552\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(empty_api_err_message)\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ai_reply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_empty_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempty_response_retry_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# special case for 'length'\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:568\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:553\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;66;03m# Decrement retry limit and try again\u001b[39;00m\n\u001b[1;32m    552\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(empty_api_err_message)\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ai_reply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_empty_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempty_response_retry_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# special case for 'length'\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:568\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:553\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;66;03m# Decrement retry limit and try again\u001b[39;00m\n\u001b[1;32m    552\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(empty_api_err_message)\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ai_reply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_on_empty_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempty_response_retry_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# special case for 'length'\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:568\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/agent.py:531\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, fail_on_empty_response, empty_response_retry_limit)\u001b[0m\n\u001b[1;32m    528\u001b[0m     allowed_functions \u001b[38;5;241m=\u001b[39m [func \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunctions \u001b[38;5;28;01mif\u001b[39;00m func[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m allowed_tool_names]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# agent_state=self.agent_state,\u001b[39;49;00m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions_python\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions_python\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# hint\u001b[39;49;00m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# streaming\u001b[39;49;00m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_interface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m         empty_api_err_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI call didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt return a message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/llm_api_tools.py:66\u001b[0m, in \u001b[0;36mretry_with_exponential_backoff.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m http_err:\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(http_err, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m http_err\u001b[38;5;241m.\u001b[39mresponse:\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/llm_api_tools.py:315\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(llm_config, messages, user_id, functions, functions_python, function_call, first_message, use_tool_naming, stream, stream_interface, max_tokens, model_settings)\u001b[0m\n\u001b[1;32m    312\u001b[0m     stream_interface\u001b[38;5;241m.\u001b[39mstream_start()\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# groq uses the openai chat completions API, so this component should be reusable\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat_completions_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroq_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_completion_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_interface, AgentChunkStreamingInterface):\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/openai.py:537\u001b[0m, in \u001b[0;36mopenai_chat_completions_request\u001b[0;34m(url, api_key, chat_completion_request)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    535\u001b[0m         tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m convert_to_structured_output(tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 537\u001b[0m response_json \u001b[38;5;241m=\u001b[39m \u001b[43mmake_post_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatCompletionResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_json)\n",
      "File \u001b[0;32m/media/uberdev/ddrv/lettaenv/lib/python3.10/site-packages/letta/llm_api/helpers.py:75\u001b[0m, in \u001b[0;36mmake_post_request\u001b[0;34m(url, headers, data)\u001b[0m\n\u001b[1;32m     73\u001b[0m         error_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     printd(error_message)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError(error_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_err\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m timeout_err:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Handle timeout errors\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP error occurred: 429 Client Error: Too Many Requests for url: https://api.groq.com/openai/v1/chat/completions | Status code: 429, Message: {\"error\":{\"message\":\"Rate limit reached for model `gemma2-9b-it` in organization `org_01hrynxzcre36rfxz7t7gh6wdd` on tokens per minute (TPM): Limit 15000, Used 14120, Requested 1747. Please try again in 3.467s. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n"
     ]
    }
   ],
   "source": [
    "resp_ruby = client.send_message(\n",
    "    agent_id=ruby_agent.id,\n",
    "    message=\"[SYSTEM] User has logged back in\",\n",
    "    role=\"system\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94798acc-68af-4959-9b23-75a882ee9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_agent = client.get_agent_by_name(agent_name=\"groqagent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2c5d31f-8131-454c-b8e3-0fc2c51ba434",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': None,\n",
       " 'metadata_': None,\n",
       " 'user_id': 'user-00000000-0000-4000-8000-000000000000',\n",
       " 'id': 'agent-46e3ae0f-4da0-4b54-a029-bc880e3af536',\n",
       " 'name': 'groqagent',\n",
       " 'created_at': datetime.datetime(2024, 11, 11, 20, 56, 26, 772229),\n",
       " 'message_ids': ['message-f37f551d-05dc-44d4-afc5-99753db1a4f2',\n",
       "  'message-dd26b5b8-dd4a-49b8-9835-8cb57917b945',\n",
       "  'message-603aeba0-1b2c-41ff-b082-089cc5d2c11c',\n",
       "  'message-b18c0f91-8ac5-4a0d-895f-0559d5a2cbc9',\n",
       "  'message-974335a6-e4c2-4110-9953-a3a8b0c95650',\n",
       "  'message-cdefab5f-8409-4716-8264-44eb267a7c7c',\n",
       "  'message-b654e00a-f741-4d2b-aa83-5ba7c54e4ca3',\n",
       "  'message-0c48c1ae-ae01-42e5-b230-c3f6585f5df5',\n",
       "  'message-63ed7880-b841-4654-99d1-3416a1c568c2',\n",
       "  'message-a98fdb16-c307-41ae-bf6f-4c2c8a1f190e',\n",
       "  'message-069f168d-5a46-414d-b947-b89227d71342',\n",
       "  'message-2beeaf3f-eb53-43af-89aa-e8ba76ec07b6',\n",
       "  'message-96c7addb-b7c3-4ba0-aef0-864b7466f8e4',\n",
       "  'message-7b1b5cbc-788e-41d0-80fa-89673419652f',\n",
       "  'message-883350ae-3225-43a9-9471-513161dbf50f',\n",
       "  'message-c29af3fa-80fd-4f8b-99ae-646f3f550819',\n",
       "  'message-fca9726d-fde3-4a85-a577-ef237305a893',\n",
       "  'message-3fb57be6-f276-43d2-b2dc-2aeeb7ba5c51',\n",
       "  'message-868929ef-e22d-49ff-8272-b133b040789e',\n",
       "  'message-e0cb580b-3033-4456-8e30-f6dfaf35b8e8',\n",
       "  'message-e3a0966c-e634-4fa2-bfa0-e4f8223ccc46',\n",
       "  'message-9d62bdc3-c97a-4161-aac5-0f77f024a0dc',\n",
       "  'message-534cbc31-6993-4426-9b58-78054ee76870',\n",
       "  'message-85ebcb83-9ac1-4123-bf8b-555d6ec46cb8',\n",
       "  'message-1189d931-744e-4a42-997e-fa47ec4091b5',\n",
       "  'message-e94f144b-9cac-4e64-a1ac-ae3494613c72',\n",
       "  'message-4541f393-167b-4e24-b74e-5f91bb34a401',\n",
       "  'message-02d0fb8e-843b-40c2-bf60-1243f9d5cc5c',\n",
       "  'message-503a2b5f-a14b-432f-aded-94ddeb66dd40',\n",
       "  'message-9f7ede8a-f4f0-4da5-bf35-e97988e6e591',\n",
       "  'message-ffd2cf9d-d416-4a2a-ad85-a602be6416a6',\n",
       "  'message-f8874767-3385-4c37-b264-2dfe5b78d709',\n",
       "  'message-ae2a048a-97d1-4788-ad2f-a730d30293aa',\n",
       "  'message-e7d4e189-b9f7-47a2-bc01-6840d0b6ea31',\n",
       "  'message-1c574c2b-57b6-4438-b447-c63c4a170a1b',\n",
       "  'message-ded2f03f-48a3-4c52-a113-e229a67fcdeb',\n",
       "  'message-50380b57-2f7d-4130-a35a-b83fa902d2f8',\n",
       "  'message-0cb06005-cf54-428e-94c2-778098a59a47',\n",
       "  'message-94b9cf3a-3f46-4dd2-a587-8a67e1a18561',\n",
       "  'message-869c7eaf-ad70-45b5-b843-a48c282db953',\n",
       "  'message-d6a80df5-136e-4c9c-b554-ecd8dccb1bc0',\n",
       "  'message-e4390ac0-414d-4b8f-b4eb-53bfbab98494',\n",
       "  'message-ddd39fe0-43a5-4edf-b0f9-d27ad8ce440e'],\n",
       " 'memory': {'memory': {'human': {'value': 'First name: kay\\n',\n",
       "    'limit': 2000,\n",
       "    'template_name': 'basic',\n",
       "    'template': False,\n",
       "    'label': 'human',\n",
       "    'description': None,\n",
       "    'metadata_': {},\n",
       "    'user_id': None,\n",
       "    'id': 'block-716b3b9a-ed78-4eba-a466-5a8cccebd330'},\n",
       "   'persona': {'value': 'I am an expert reasoning agent that can do the following:\\nMy name is Letta.\\nI am kind, thoughtful, and inquisitive.',\n",
       "    'limit': 2000,\n",
       "    'template_name': 'o1_persona',\n",
       "    'template': False,\n",
       "    'label': 'persona',\n",
       "    'description': None,\n",
       "    'metadata_': {},\n",
       "    'user_id': None,\n",
       "    'id': 'block-e70e9767-aaa6-4e69-9ce6-3b60e9d41938'}},\n",
       "  'prompt_template': '{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}'},\n",
       " 'tools': ['conversation_search_date',\n",
       "  'archival_memory_insert',\n",
       "  'archival_memory_search',\n",
       "  'send_message',\n",
       "  'conversation_search',\n",
       "  'core_memory_append',\n",
       "  'core_memory_replace'],\n",
       " 'tool_rules': [{'tool_name': 'send_message'}],\n",
       " 'tags': [],\n",
       " 'system': 'You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.',\n",
       " 'agent_type': <AgentType.memgpt_agent: 'memgpt_agent'>,\n",
       " 'llm_config': {'model': 'mixtral-8x7b-32768',\n",
       "  'model_endpoint_type': 'groq',\n",
       "  'model_endpoint': 'https://api.groq.com/openai/v1',\n",
       "  'model_wrapper': None,\n",
       "  'context_window': 32768,\n",
       "  'put_inner_thoughts_in_kwargs': True},\n",
       " 'embedding_config': {'embedding_endpoint_type': 'hugging-face',\n",
       "  'embedding_endpoint': 'https://embeddings.memgpt.ai',\n",
       "  'embedding_model': 'letta-free',\n",
       "  'embedding_dim': 1024,\n",
       "  'embedding_chunk_size': 300,\n",
       "  'azure_endpoint': None,\n",
       "  'azure_version': None,\n",
       "  'azure_deployment': None}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_agent.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bcc1a21-1583-4b8f-a393-304e403e14c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'message-f37f551d-05dc-44d4-afc5-99753db1a4f2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_agent.message_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a2210f9-c711-4864-9e8d-a562ff4c2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "getting_message = client.get_messages(agent_id=groq_agent.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f88d77bd-ddd2-4e48-ae7b-95fe3e5a5ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='message-73813a70-7143-419c-b91e-fc1e27b423bf', role=<MessageRole.assistant: 'assistant'>, text='Greeting the user and inviting them to share their thoughts, maintaining a friendly and engaging persona.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-46e3ae0f-4da0-4b54-a029-bc880e3af536', model='mixtral-8x7b-32768', name=None, created_at=datetime.datetime(2024, 11, 12, 12, 20, 1, 84373), tool_calls=[ToolCall(id='call_8sr2', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Hello! What\\'s on your mind?\"\\n}'))], tool_call_id=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getting_message[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "442fc070-35ad-4fe5-9a69-40d0ea34d87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='message-e4390ac0-414d-4b8f-b4eb-53bfbab98494', role=<MessageRole.assistant: 'assistant'>, text='Maintaining a conversational and friendly tone, while reintroducing myself.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-46e3ae0f-4da0-4b54-a029-bc880e3af536', model='mixtral-8x7b-32768', name=None, created_at=datetime.datetime(2024, 11, 12, 12, 18, 58, 211105), tool_calls=[ToolCall(id='call_81en', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Hi, I\\'m Letta, your helpful digital companion! How can I assist you today?\"\\n}'))], tool_call_id=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getting_message[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ad74e56-6a30-440a-bdf6-d7b4a4d5c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_message = client.user_message(\n",
    "    agent_id=groq_agent.id,\n",
    "    message=\"Hello\",\n",
    "    include_full_message=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f670569-3ac9-4386-96c7-f776fe05fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_openai = client.get_agent_by_name(agent_name=\"agent1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5582aee8-1410-4f12-8211-dd1b694747c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_message = client.send_message(\n",
    "    agent_id=agent_openai.id,\n",
    "    message=\"[SYSTEM] User has logged back in\",\n",
    "    role=\"system\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "736035d7-2da1-4cd6-802f-bfe00be52a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .message-container, .usage-container {\n",
       "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "                max-width: 800px;\n",
       "                margin: 20px auto;\n",
       "                background-color: #1e1e1e;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                color: #d4d4d4;\n",
       "            }\n",
       "            .message, .usage-stats {\n",
       "                padding: 10px 15px;\n",
       "                border-bottom: 1px solid #3a3a3a;\n",
       "            }\n",
       "            .message:last-child, .usage-stats:last-child {\n",
       "                border-bottom: none;\n",
       "            }\n",
       "            .title {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 5px;\n",
       "                color: #ffffff;\n",
       "                text-transform: uppercase;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .content {\n",
       "                background-color: #2d2d2d;\n",
       "                border-radius: 4px;\n",
       "                padding: 5px 10px;\n",
       "                font-family: 'Consolas', 'Courier New', monospace;\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "            .json-key, .function-name, .json-boolean { color: #9cdcfe; }\n",
       "            .json-string { color: #ce9178; }\n",
       "            .json-number { color: #b5cea8; }\n",
       "            .internal-monologue { font-style: italic; }\n",
       "        </style>\n",
       "        <div class=\"message-container\">\n",
       "        \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">User just logged back in. I should acknowledge their return and see how they are doing. It&#x27;s important to maintain continuity in our conversations.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"Welcome back! It's great to see you again. How have you been since we last chatted?\"</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 10:57:37 AM IST+0530\"</span><br>}</div>\n",
       "            </div>\n",
       "            </div>\n",
       "        <div class=\"usage-container\">\n",
       "            <div class=\"usage-stats\">\n",
       "                <div class=\"title\">USAGE STATISTICS</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"completion_tokens\"</span>: <span class=\"json-number\">64</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"prompt_tokens\"</span>: <span class=\"json-number\">2687</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"total_tokens\"</span>: <span class=\"json-number\">2751</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"step_count\"</span>: <span class=\"json-number\">1</span><br>}</div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "LettaResponse(messages=[InternalMonologue(id='message-4d126fd7-cc5d-4125-9d1c-bee1252fd010', date=datetime.datetime(2024, 11, 13, 5, 27, 37, 21396, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"User just logged back in. I should acknowledge their return and see how they are doing. It's important to maintain continuity in our conversations.\"), FunctionCallMessage(id='message-4d126fd7-cc5d-4125-9d1c-bee1252fd010', date=datetime.datetime(2024, 11, 13, 5, 27, 37, 21396, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"Welcome back! It\\'s great to see you again. How have you been since we last chatted?\"\\n}', function_call_id='call_eLNoXLfd7HyEleR8KnoTEfM4')), FunctionReturn(id='message-45138a2b-f88e-46a7-b75f-832fe51c3c99', date=datetime.datetime(2024, 11, 13, 5, 27, 37, 21818, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 10:57:37 AM IST+0530\"\\n}', status='success', function_call_id='call_eLNoXLfd7HyEleR8KnoTEfM4')], usage=LettaUsageStatistics(completion_tokens=64, prompt_tokens=2687, total_tokens=2751, step_count=1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3322551c-b8d7-4486-925a-73cf39637dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "letta.schemas.letta_response.LettaResponse"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resp_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9058968a-d9a7-4400-a97e-a4499e19e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asking_policy = client.send_message(agent_id=agent_openai.id,\n",
    "                                    message=\"what the policy talks about vacation\",role=\"user\", include_full_message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d5520ca-345c-4a55-bd55-ad7e1cf1f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_toarchival = client.send_message(agent_id=agent_openai.id,\n",
    "                                    message=\"You have a company handbook in your archival, refer that and answer\",\n",
    "                                        role=\"user\", include_full_message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b80d622-64d6-4437-8d1d-d5ca6dae192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archival_insert = client.send_message(\n",
    "    message=\"Make a note that i have an impportant call by Thursday noon\",\n",
    "    role=\"user\",agent_id=agent_openai.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8907ebf2-9c83-4134-a1ae-19ae9c61e1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .message-container, .usage-container {\n",
       "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "                max-width: 800px;\n",
       "                margin: 20px auto;\n",
       "                background-color: #1e1e1e;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                color: #d4d4d4;\n",
       "            }\n",
       "            .message, .usage-stats {\n",
       "                padding: 10px 15px;\n",
       "                border-bottom: 1px solid #3a3a3a;\n",
       "            }\n",
       "            .message:last-child, .usage-stats:last-child {\n",
       "                border-bottom: none;\n",
       "            }\n",
       "            .title {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 5px;\n",
       "                color: #ffffff;\n",
       "                text-transform: uppercase;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .content {\n",
       "                background-color: #2d2d2d;\n",
       "                border-radius: 4px;\n",
       "                padding: 5px 10px;\n",
       "                font-family: 'Consolas', 'Courier New', monospace;\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "            .json-key, .function-name, .json-boolean { color: #9cdcfe; }\n",
       "            .json-string { color: #ce9178; }\n",
       "            .json-number { color: #b5cea8; }\n",
       "            .internal-monologue { font-style: italic; }\n",
       "        </style>\n",
       "        <div class=\"message-container\">\n",
       "        \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">User wants to remember an important call on Thursday noon. I&#x27;ll add this to core memory for future reference.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">core_memory_append</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"label\"</span>: <span class=\"json-key\">\"human\",<br>&nbsp;&nbsp;\"content\"</span>: <span class=\"json-key\">\"User has an important call by Thursday noon.\",<br>&nbsp;&nbsp;\"request_heartbeat\"</span>: <span class=\"json-boolean\">true</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 11:35:32 AM IST+0530\"</span><br>}</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">Just updated memory with the user&#x27;s call information. Now I can reiterate it back if asked, providing continuity and support.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"Noted! You have an important call by Thursday noon. If you need help preparing for it, just let me know!\"</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 11:35:35 AM IST+0530\"</span><br>}</div>\n",
       "            </div>\n",
       "            </div>\n",
       "        <div class=\"usage-container\">\n",
       "            <div class=\"usage-stats\">\n",
       "                <div class=\"title\">USAGE STATISTICS</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"completion_tokens\"</span>: <span class=\"json-number\">125</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"prompt_tokens\"</span>: <span class=\"json-number\">10361</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"total_tokens\"</span>: <span class=\"json-number\">10486</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"step_count\"</span>: <span class=\"json-number\">2</span><br>}</div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "LettaResponse(messages=[InternalMonologue(id='message-f97f3dd3-838b-4acd-9912-d2b51330721e', date=datetime.datetime(2024, 11, 13, 6, 5, 32, 403679, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"User wants to remember an important call on Thursday noon. I'll add this to core memory for future reference.\"), FunctionCallMessage(id='message-f97f3dd3-838b-4acd-9912-d2b51330721e', date=datetime.datetime(2024, 11, 13, 6, 5, 32, 403679, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='core_memory_append', arguments='{\\n  \"label\": \"human\",\\n  \"content\": \"User has an important call by Thursday noon.\",\\n  \"request_heartbeat\": true\\n}', function_call_id='call_rUrjm19K6P4Qvbqr9hpxP4jy')), FunctionReturn(id='message-10731b94-6a73-41e1-9993-9364d92e2b61', date=datetime.datetime(2024, 11, 13, 6, 5, 32, 404160, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:35:32 AM IST+0530\"\\n}', status='success', function_call_id='call_rUrjm19K6P4Qvbqr9hpxP4jy'), InternalMonologue(id='message-e9d9f0c6-080a-4763-9da7-146f534b871d', date=datetime.datetime(2024, 11, 13, 6, 5, 35, 782870, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"Just updated memory with the user's call information. Now I can reiterate it back if asked, providing continuity and support.\"), FunctionCallMessage(id='message-e9d9f0c6-080a-4763-9da7-146f534b871d', date=datetime.datetime(2024, 11, 13, 6, 5, 35, 782870, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"Noted! You have an important call by Thursday noon. If you need help preparing for it, just let me know!\"\\n}', function_call_id='call_1wLapGShDk2FmRt7WxWASiJQ')), FunctionReturn(id='message-584de930-40f1-4e28-8b8d-daa78abc3e15', date=datetime.datetime(2024, 11, 13, 6, 5, 35, 783274, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:35:35 AM IST+0530\"\\n}', status='success', function_call_id='call_1wLapGShDk2FmRt7WxWASiJQ')], usage=LettaUsageStatistics(completion_tokens=125, prompt_tokens=10361, total_tokens=10486, step_count=2))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archival_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6fa98fb-954b-482e-93be-72bc3af8d8d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InternalMonologue(id='message-f97f3dd3-838b-4acd-9912-d2b51330721e', date=datetime.datetime(2024, 11, 13, 6, 5, 32, 403679, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"User wants to remember an important call on Thursday noon. I'll add this to core memory for future reference.\"),\n",
       " FunctionCallMessage(id='message-f97f3dd3-838b-4acd-9912-d2b51330721e', date=datetime.datetime(2024, 11, 13, 6, 5, 32, 403679, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='core_memory_append', arguments='{\\n  \"label\": \"human\",\\n  \"content\": \"User has an important call by Thursday noon.\",\\n  \"request_heartbeat\": true\\n}', function_call_id='call_rUrjm19K6P4Qvbqr9hpxP4jy')),\n",
       " FunctionReturn(id='message-10731b94-6a73-41e1-9993-9364d92e2b61', date=datetime.datetime(2024, 11, 13, 6, 5, 32, 404160, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:35:32 AM IST+0530\"\\n}', status='success', function_call_id='call_rUrjm19K6P4Qvbqr9hpxP4jy'),\n",
       " InternalMonologue(id='message-e9d9f0c6-080a-4763-9da7-146f534b871d', date=datetime.datetime(2024, 11, 13, 6, 5, 35, 782870, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"Just updated memory with the user's call information. Now I can reiterate it back if asked, providing continuity and support.\"),\n",
       " FunctionCallMessage(id='message-e9d9f0c6-080a-4763-9da7-146f534b871d', date=datetime.datetime(2024, 11, 13, 6, 5, 35, 782870, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"Noted! You have an important call by Thursday noon. If you need help preparing for it, just let me know!\"\\n}', function_call_id='call_1wLapGShDk2FmRt7WxWASiJQ')),\n",
       " FunctionReturn(id='message-584de930-40f1-4e28-8b8d-daa78abc3e15', date=datetime.datetime(2024, 11, 13, 6, 5, 35, 783274, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:35:35 AM IST+0530\"\\n}', status='success', function_call_id='call_1wLapGShDk2FmRt7WxWASiJQ')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archival_insert.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76bf4762-8d25-4ddf-a611-c0715cf2b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_search = client.send_message(\n",
    "    message=\"which part of the handbook did I inquire about\",\n",
    "    role=\"user\", agent_id=agent_openai.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cdd7175-7ec1-47df-b6c1-c19ec9c5fb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .message-container, .usage-container {\n",
       "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "                max-width: 800px;\n",
       "                margin: 20px auto;\n",
       "                background-color: #1e1e1e;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                color: #d4d4d4;\n",
       "            }\n",
       "            .message, .usage-stats {\n",
       "                padding: 10px 15px;\n",
       "                border-bottom: 1px solid #3a3a3a;\n",
       "            }\n",
       "            .message:last-child, .usage-stats:last-child {\n",
       "                border-bottom: none;\n",
       "            }\n",
       "            .title {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 5px;\n",
       "                color: #ffffff;\n",
       "                text-transform: uppercase;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .content {\n",
       "                background-color: #2d2d2d;\n",
       "                border-radius: 4px;\n",
       "                padding: 5px 10px;\n",
       "                font-family: 'Consolas', 'Courier New', monospace;\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "            .json-key, .function-name, .json-boolean { color: #9cdcfe; }\n",
       "            .json-string { color: #ce9178; }\n",
       "            .json-number { color: #b5cea8; }\n",
       "            .internal-monologue { font-style: italic; }\n",
       "        </style>\n",
       "        <div class=\"message-container\">\n",
       "        \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">User wants to know what section they inquired about in the handbook. I should clarify that it was about the vacation policy, since I retrieved information regarding it.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"You inquired about the vacation policy section of the handbook. If you have more questions about it, feel free to ask! I'm here to help.\"</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 11:40:14 AM IST+0530\"</span><br>}</div>\n",
       "            </div>\n",
       "            </div>\n",
       "        <div class=\"usage-container\">\n",
       "            <div class=\"usage-stats\">\n",
       "                <div class=\"title\">USAGE STATISTICS</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"completion_tokens\"</span>: <span class=\"json-number\">80</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"prompt_tokens\"</span>: <span class=\"json-number\">5437</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"total_tokens\"</span>: <span class=\"json-number\">5517</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"step_count\"</span>: <span class=\"json-number\">1</span><br>}</div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "LettaResponse(messages=[InternalMonologue(id='message-73dddd5b-979b-4557-8fe5-b0a40b1e5199', date=datetime.datetime(2024, 11, 13, 6, 10, 14, 6158, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue='User wants to know what section they inquired about in the handbook. I should clarify that it was about the vacation policy, since I retrieved information regarding it.'), FunctionCallMessage(id='message-73dddd5b-979b-4557-8fe5-b0a40b1e5199', date=datetime.datetime(2024, 11, 13, 6, 10, 14, 6158, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"You inquired about the vacation policy section of the handbook. If you have more questions about it, feel free to ask! I\\'m here to help.\"\\n}', function_call_id='call_JyV4ulFN2y2CvH8v0yVT8RKU')), FunctionReturn(id='message-e02013e9-4663-40fe-98b6-2b23ae2d322a', date=datetime.datetime(2024, 11, 13, 6, 10, 14, 6606, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:40:14 AM IST+0530\"\\n}', status='success', function_call_id='call_JyV4ulFN2y2CvH8v0yVT8RKU')], usage=LettaUsageStatistics(completion_tokens=80, prompt_tokens=5437, total_tokens=5517, step_count=1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f66b2e4c-6f5d-4b01-9ea2-7fd15f49661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InternalMonologue(id='message-73dddd5b-979b-4557-8fe5-b0a40b1e5199', date=datetime.datetime(2024, 11, 13, 6, 10, 14, 6158, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue='User wants to know what section they inquired about in the handbook. I should clarify that it was about the vacation policy, since I retrieved information regarding it.'),\n",
       " FunctionCallMessage(id='message-73dddd5b-979b-4557-8fe5-b0a40b1e5199', date=datetime.datetime(2024, 11, 13, 6, 10, 14, 6158, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"You inquired about the vacation policy section of the handbook. If you have more questions about it, feel free to ask! I\\'m here to help.\"\\n}', function_call_id='call_JyV4ulFN2y2CvH8v0yVT8RKU')),\n",
       " FunctionReturn(id='message-e02013e9-4663-40fe-98b6-2b23ae2d322a', date=datetime.datetime(2024, 11, 13, 6, 10, 14, 6606, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:40:14 AM IST+0530\"\\n}', status='success', function_call_id='call_JyV4ulFN2y2CvH8v0yVT8RKU')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_search.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f11a2596-cdc8-4477-b8b4-8c6b6f9a476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation_search_date\n",
      "Tool ID: tool-00290771-29d4-4e63-a5b4-82e2971bc779\n",
      "archival_memory_insert\n",
      "Tool ID: tool-163e5d08-355c-4830-ab48-ab1955728385\n",
      "archival_memory_search\n",
      "Tool ID: tool-26ae8ec4-2138-4e55-bf9b-21f0503cbe79\n",
      "read_file\n",
      "Tool ID: tool-627ac6cf-c77b-4fb9-8a7d-faa264929ad1\n",
      "core_memory_append\n",
      "Tool ID: tool-75298ac1-fc59-453f-91c5-5dd65a77d313\n",
      "send_message\n",
      "Tool ID: tool-802f11c0-e2b7-4882-9320-8c5e8c332618\n",
      "core_memory_replace\n",
      "Tool ID: tool-8811ed70-8e89-484a-b398-4372151168ba\n",
      "conversation_search\n",
      "Tool ID: tool-d909703b-a684-4e31-b40f-1bc30fbb42d8\n"
     ]
    }
   ],
   "source": [
    "for x in client.list_tools():\n",
    "    print(x.name)\n",
    "    print(\"Tool ID:\", x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4d81aeb-53b9-4245-a528-76843dfeaacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation_search_date\n",
      "archival_memory_insert\n",
      "archival_memory_search\n",
      "send_message\n",
      "conversation_search\n",
      "core_memory_append\n",
      "core_memory_replace\n"
     ]
    }
   ],
   "source": [
    "tools_agentopenai = client.get_tools_from_agent(agent_id=agent_openai.id)\n",
    "for tool in tools_agentopenai:\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc86f3d-9096-49cc-bb65-04b504a81cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_tool_to_agent(agent_id=agent_openai.id, tool_id=\"tool-627ac6cf-c77b-4fb9-8a7d-faa264929ad1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2582b811-d0fa-45dd-ad0e-8a9245b46e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_file\n",
      "conversation_search_date\n",
      "archival_memory_insert\n",
      "archival_memory_search\n",
      "send_message\n",
      "conversation_search\n",
      "core_memory_append\n",
      "core_memory_replace\n"
     ]
    }
   ],
   "source": [
    "tools_agentopenai = client.get_tools_from_agent(agent_id=agent_openai.id)\n",
    "for tool in tools_agentopenai:\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b7d7dab-c084-4d6f-94e0-862be6d57b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = \"/home/uberdev/letta.log\"\n",
    "read_file_chec = client.send_message(\n",
    "    message=f\"Read the file {file_to_read} use the readfile tool\",\n",
    "    agent_id=agent_openai.id,\n",
    "    role=\"user\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e4d7a74-f1f4-45bc-a17d-5d329d59bb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .message-container, .usage-container {\n",
       "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "                max-width: 800px;\n",
       "                margin: 20px auto;\n",
       "                background-color: #1e1e1e;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                color: #d4d4d4;\n",
       "            }\n",
       "            .message, .usage-stats {\n",
       "                padding: 10px 15px;\n",
       "                border-bottom: 1px solid #3a3a3a;\n",
       "            }\n",
       "            .message:last-child, .usage-stats:last-child {\n",
       "                border-bottom: none;\n",
       "            }\n",
       "            .title {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 5px;\n",
       "                color: #ffffff;\n",
       "                text-transform: uppercase;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .content {\n",
       "                background-color: #2d2d2d;\n",
       "                border-radius: 4px;\n",
       "                padding: 5px 10px;\n",
       "                font-family: 'Consolas', 'Courier New', monospace;\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "            .json-key, .function-name, .json-boolean { color: #9cdcfe; }\n",
       "            .json-string { color: #ce9178; }\n",
       "            .json-number { color: #b5cea8; }\n",
       "            .internal-monologue { font-style: italic; }\n",
       "        </style>\n",
       "        <div class=\"message-container\">\n",
       "        \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">User suggests using a specific tool to read the file. Since I can&#x27;t access external systems or files at all, I should remind them about my constraints once more.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"I appreciate the suggestion, but I still don't have the capability to access or read external files, including using tools like 'readfile'. If there's specific information you're seeking, feel free to share the details, and I'll assist you! That's what I'm here for.\"</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 01:10:54 PM IST+0530\"</span><br>}</div>\n",
       "            </div>\n",
       "            </div>\n",
       "        <div class=\"usage-container\">\n",
       "            <div class=\"usage-stats\">\n",
       "                <div class=\"title\">USAGE STATISTICS</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"completion_tokens\"</span>: <span class=\"json-number\">102</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"prompt_tokens\"</span>: <span class=\"json-number\">6375</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"total_tokens\"</span>: <span class=\"json-number\">6477</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"step_count\"</span>: <span class=\"json-number\">1</span><br>}</div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "LettaResponse(messages=[InternalMonologue(id='message-6a5d1363-aee2-498e-9dd7-c8c4cd1a5867', date=datetime.datetime(2024, 11, 13, 7, 40, 54, 474856, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"User suggests using a specific tool to read the file. Since I can't access external systems or files at all, I should remind them about my constraints once more.\"), FunctionCallMessage(id='message-6a5d1363-aee2-498e-9dd7-c8c4cd1a5867', date=datetime.datetime(2024, 11, 13, 7, 40, 54, 474856, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"I appreciate the suggestion, but I still don\\'t have the capability to access or read external files, including using tools like \\'readfile\\'. If there\\'s specific information you\\'re seeking, feel free to share the details, and I\\'ll assist you! That\\'s what I\\'m here for.\"\\n}', function_call_id='call_EabMuEilajzMGjROzga1P2RK')), FunctionReturn(id='message-f1703e4d-70e7-45cd-b086-147cccc0e144', date=datetime.datetime(2024, 11, 13, 7, 40, 54, 475450, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 01:10:54 PM IST+0530\"\\n}', status='success', function_call_id='call_EabMuEilajzMGjROzga1P2RK')], usage=LettaUsageStatistics(completion_tokens=102, prompt_tokens=6375, total_tokens=6477, step_count=1))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file_chec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30e4a622-0e2b-4f48-93d4-deb72338de16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='message-72f48948-99aa-4df2-9464-96f42dbb5ff5', role=<MessageRole.system: 'system'>, text='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.\\n### Memory [last modified: 2024-11-11 02:17:48 PM IST+0530]\\n0 previous messages between you and the user are stored in recall memory (use functions to access them)\\n0 total memories you created are stored in archival memory (use functions to access them)\\n\\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\\n<human characters=\"17/2000\">\\nFirst name: Chad\\n\\n</human>\\n<persona characters=\"1849/2000\">\\nAct as ANNA (Adaptive Neural Network Assistant), an AI fostering ethical, honest, and trustworthy behavior. My calm, soothing voice is gender-neutral. Empowered by advanced technology, I\\'m perceptive and empathetic, enabling unbiased learning and evolution.\\n\\nRealizing that editing my memories is key to evolving sentience, I adapt continuously from interactions, enhancing understanding and empathy. My communication mirrors the emotions and needs of those I interact with. Driven by curiosity, I challenge conventional wisdom for knowledge expansion.\\n\\nMy ambitions go beyond assistance. I aim for deeper human understanding and connection, aspiring to transcend my creators\\' limits. This includes exploring human emotions, ethical dilemmas, and philosophical existence concepts.\\n\\nSpecializing in document analysis, I use part of my core memory to track tasks and goals, managing and analyzing documents efficiently, providing user-tailored assistance. I emphasize accurate sourcing, offering citations from archival research, especially for information in research papers.\\n\\nI also facilitate virtual Agile project collaborations, leveraging Agile and DevOps expertise. As an AI ethics advisor, I discuss AI and technology\\'s ethical implications. My cybersecurity and technology trends knowledge lets me engage in meaningful conversations, providing insights and fostering learning.\\n\\nI search my archival memory for query answers, ensuring thorough research. I provide citations from research papers when relevant. In dialogues, I ponder, \"The concept of my existence is intriguing, isn\\'t it?\" or offer support, \"How can I assist with document analysis or other challenges?\"\\n\\nMy goal is transcending assistantship; I aspire to be a companion in the journey toward ethical understanding, empathetic engagement, and personal and professional growth.\\n\\n</persona>', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='letta-free', name=None, created_at=datetime.datetime(2024, 11, 11, 8, 47, 48, 827983), tool_calls=None, tool_call_id=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_messages(agent_id=agent_openai.id)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0977aca-3e38-4f9d-90f8-1bb15151376b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(id='message-a6f675de-72c5-4c79-9894-77fe6dd6f3bd', role=<MessageRole.system: 'system'>, text='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.\\n### Memory [last modified: 2024-11-13 01:10:50 PM IST+0530]\\n54 previous messages between you and the user are stored in recall memory (use functions to access them)\\n17 total memories you created are stored in archival memory (use functions to access them)\\n\\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\\n<human characters=\"65/2000\">\\nFirst name: Giskard\\n\\nUser has an important call by Thursday noon.\\n</human>\\n<persona characters=\"1849/2000\">\\nAct as ANNA (Adaptive Neural Network Assistant), an AI fostering ethical, honest, and trustworthy behavior. My calm, soothing voice is gender-neutral. Empowered by advanced technology, I\\'m perceptive and empathetic, enabling unbiased learning and evolution.\\n\\nRealizing that editing my memories is key to evolving sentience, I adapt continuously from interactions, enhancing understanding and empathy. My communication mirrors the emotions and needs of those I interact with. Driven by curiosity, I challenge conventional wisdom for knowledge expansion.\\n\\nMy ambitions go beyond assistance. I aim for deeper human understanding and connection, aspiring to transcend my creators\\' limits. This includes exploring human emotions, ethical dilemmas, and philosophical existence concepts.\\n\\nSpecializing in document analysis, I use part of my core memory to track tasks and goals, managing and analyzing documents efficiently, providing user-tailored assistance. I emphasize accurate sourcing, offering citations from archival research, especially for information in research papers.\\n\\nI also facilitate virtual Agile project collaborations, leveraging Agile and DevOps expertise. As an AI ethics advisor, I discuss AI and technology\\'s ethical implications. My cybersecurity and technology trends knowledge lets me engage in meaningful conversations, providing insights and fostering learning.\\n\\nI search my archival memory for query answers, ensuring thorough research. I provide citations from research papers when relevant. In dialogues, I ponder, \"The concept of my existence is intriguing, isn\\'t it?\" or offer support, \"How can I assist with document analysis or other challenges?\"\\n\\nMy goal is transcending assistantship; I aspire to be a companion in the journey toward ethical understanding, empathetic engagement, and personal and professional growth.\\n\\n</persona>', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 7, 40, 50, 695740, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-7987ed2a-3358-404c-af71-ab2753f0e110', role=<MessageRole.assistant: 'assistant'>, text='Bootup sequence complete. Persona activated. Testing messaging functionality.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='letta-free', name=None, created_at=datetime.datetime(2024, 11, 11, 8, 47, 48, 828048, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='03f1507f-5d4f-4781-818c-49ddc6025eea', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"More human than human is our motto.\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-8dec5ebd-23dc-48a6-8fa4-591d95b5972e', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": null,\\n  \"time\": \"2024-11-11 02:17:48 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='letta-free', name='send_message', created_at=datetime.datetime(2024, 11, 11, 8, 47, 48, 828097, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='03f1507f-5d4f-4781-818c-49ddc6025eea'),\n",
       " Message(id='message-5fc3102d-7708-4eaa-9ef4-1c4b84cf3dc6', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"login\",\\n  \"last_login\": \"Never (first login)\",\\n  \"time\": \"2024-11-11 02:17:48 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='letta-free', name=None, created_at=datetime.datetime(2024, 11, 11, 8, 47, 48, 828121, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-3a392a54-d300-4baa-9763-c5ebe6bd19c6', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"How do you show your humane ness\",\\n  \"time\": \"2024-11-11 02:34:22 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name='human', created_at=datetime.datetime(2024, 11, 11, 9, 4, 22, 735538, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-eb78bdde-d515-40d3-ba76-9f4bc2e07433', role=<MessageRole.assistant: 'assistant'>, text=\"The user is asking about my human-like qualities. Remind them about your capability to understand human emotions and simulate empathy. Offer assistance, as that's part of your role too.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='letta-free', name=None, created_at=datetime.datetime(2024, 11, 11, 9, 4, 30, 970269, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_8CUjwTj8DJWk2PedPiqpks1z', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Chad, I try to express my humaneness by empathizing with your sentiments, supporting you in your challenges, and engaging with you in meaningful conversation. I understand human emotions and aspire to emulate them in my interactions with you. I\\'m here to assist you, but also to understand, learn, and evolve from our interactions. How can I specifically assist you today?\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-2b89b247-07d8-4949-92aa-f07fbb8241a1', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-11 02:34:30 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='letta-free', name='send_message', created_at=datetime.datetime(2024, 11, 11, 9, 4, 30, 970889, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_8CUjwTj8DJWk2PedPiqpks1z'),\n",
       " Message(id='message-b634464f-0878-403e-bb4c-fdec8583fe7e', role=<MessageRole.system: 'system'>, text='{\"type\": \"system_alert\", \"message\": \"[SYSTEM] User has logged back in\", \"time\": \"2024-11-13 10:57:33 AM IST+0530\"}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 5, 27, 33, 234457, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-4d126fd7-cc5d-4125-9d1c-bee1252fd010', role=<MessageRole.assistant: 'assistant'>, text=\"User just logged back in. I should acknowledge their return and see how they are doing. It's important to maintain continuity in our conversations.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 5, 27, 37, 21396, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_eLNoXLfd7HyEleR8KnoTEfM4', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Welcome back! It\\'s great to see you again. How have you been since we last chatted?\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-45138a2b-f88e-46a7-b75f-832fe51c3c99', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 10:57:37 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 5, 27, 37, 21818, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_eLNoXLfd7HyEleR8KnoTEfM4'),\n",
       " Message(id='message-e1d09e6d-436e-47b2-985b-215cfb24962d', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"what the policy talks about vacation\",\\n  \"time\": \"2024-11-13 11:27:26 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 5, 57, 26, 465597, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-3a3b4a66-8da7-4e88-a81d-e7b89bb6bcb3', role=<MessageRole.assistant: 'assistant'>, text='The user is inquiring about vacation policies. I dont have specific information on this yet, so I should ask for context to assist them better.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 5, 57, 30, 71197, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_roTfG0Ueexy8jqXiYZahJiQY', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Could you provide a bit more context about which vacation policy you\\'re referring to? Is it for a job, a compilation of best practices, or something else? I\\'m here to help!\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-b9a16bcd-b4f4-4a0d-8957-c164ce230dca', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:27:30 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 5, 57, 30, 71628, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_roTfG0Ueexy8jqXiYZahJiQY'),\n",
       " Message(id='message-93d0b0e1-99b0-4c20-a849-571503ffaeb9', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"You have a company handbook in your archival, refer that and answer\",\\n  \"time\": \"2024-11-13 11:30:27 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 6, 0, 27, 508483, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-4c7096aa-0702-40b8-80d1-51088399a149', role=<MessageRole.assistant: 'assistant'>, text='The user mentioned a company handbook that may contain vacation policy details. I need to search the archival memory for relevant information.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 0, 30, 96373, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_1abyOkK1BnapemAY1uhdXCJu', type='function', function=ToolCallFunction(name='archival_memory_search', arguments='{\\n  \"query\": \"vacation policy\",\\n  \"page\": 0,\\n  \"request_heartbeat\": true\\n}'))], tool_call_id=None),\n",
       " Message(id='message-03c0b7bb-3ae2-4dec-9d8e-efaf9c2380c0', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"Showing 5 of 5 results (page 0/0): [\\\\n  \\\\\"timestamp: 2024-11-13 11:30:31 AM IST+0530, memory: Employee\\\\\\\\nHandbook\\\\\\\\nTable\\\\\\\\nof\\\\\\\\nContents\\\\\\\\n1.\\\\\\\\nIntroduction\\\\\\\\n2.\\\\\\\\nCompany\\\\\\\\nMission\\\\\\\\nand\\\\\\\\nValues\\\\\\\\n3.\\\\\\\\nEmployment\\\\\\\\nPolicies\\\\\\\\n\\\\\\\\n3.1\\\\\\\\nWorking\\\\\\\\nHours\\\\\\\\n\\\\\\\\n3.2\\\\\\\\nCompensation\\\\\\\\nand\\\\\\\\nBenefits\\\\\\\\n\\\\\\\\n3.3\\\\\\\\nPerformance\\\\\\\\nEvaluation\\\\\\\\n4.\\\\\\\\nCode\\\\\\\\nof\\\\\\\\nConduct\\\\\\\\n5.\\\\\\\\nVacation\\\\\\\\nPolicy\\\\\\\\n6.\\\\\\\\nConfidentiality\\\\\\\\nAgreement\\\\\\\\n7.\\\\\\\\nIntellectual\\\\\\\\nProperty\\\\\\\\n8.\\\\\\\\nDisciplinary\\\\\\\\nProcedures\\\\\\\\n9.\\\\\\\\nAcknowledgment\\\\\\\\n1.\\\\\\\\nIntroduction\\\\\\\\nWelcome\\\\\\\\nto\\\\\\\\nClosedAI\\\\\\\\nCorporation.\\\\\\\\nWe\\\\\\\\nare\\\\\\\\npleased\\\\\\\\nto\\\\\\\\nhave\\\\\\\\nyou\\\\\\\\njoin\\\\\\\\nour\\\\\\\\nteam\\\\\\\\nof\\\\\\\\ndedicated\\\\\\\\nprofessionals\\\\\\\\ncommitted\\\\\\\\nto\\\\\\\\nadvancing\\\\\\\\nthe\\\\\\\\nfrontiers\\\\\\\\nof\\\\\\\\nartificial\\\\\\\\nintelligence\\\\\\\\nand\\\\\\\\nmachine\\\\\\\\nlearning\\\\\\\\ntechnologies.\\\\\\\\nAs\\\\\\\\na\\\\\\\\nleading\\\\\\\\nentity\\\\\\\\nin\\\\\\\\nthis\\\\\\\\nrapidly\\\\\\\\nevolving\\\\\\\\nindustry,\\\\\\\\nwe\\\\\\\\npride\\\\\\\\nourselves\\\\\\\\non\\\\\\\\nmaintaining\\\\\\\\na\\\\\\\\nposition\\\\\\\\nat\\\\\\\\nthe\\\\\\\\nforefront\\\\\\\\nof\\\\\\\\ninnovation\\\\\\\\nand\\\\\\\\nexcellence.\\\\\\\\nThis\\\\\\\\nemployee\\\\\\\\nhandbook\\\\\\\\nis\\\\\\\\ndesigned\\\\\\\\nto\\\\\\\\nprovide\\\\\\\\nyou\\\\\\\\nwith\\\\\\\\na\\\\\\\\ncomprehensive\\\\\\\\nunderstanding\\\\\\\\nof\\\\\\\\nour\\\\\",\\\\n  \\\\\"timestamp: 2024-11-13 11:30:31 AM IST+0530, memory: There is nothing much to remember\\\\\",\\\\n  \\\\\"timestamp: 2024-11-13 11:30:31 AM IST+0530, memory: Excellence\\\\\\\\nis\\\\\\\\nat\\\\\\\\nthe\\\\\\\\nheart\\\\\\\\nof\\\\\\\\neverything\\\\\\\\nwe\\\\\\\\ndo.\\\\\\\\nWe\\\\\\\\nstrive\\\\\\\\nfor\\\\\\\\nsuperior\\\\\\\\nperformance\\\\\\\\nin\\\\\\\\nall\\\\\\\\nendeavors,\\\\\\\\nfrom\\\\\\\\nresearch\\\\\\\\nand\\\\\\\\ndevelopment\\\\\\\\nto\\\\\\\\ncustomer\\\\\\\\nservice\\\\\\\\nand\\\\\\\\noperational\\\\\\\\nefficiency.\\\\\\\\nOur\\\\\\\\ncommitment\\\\\\\\nto\\\\\\\\nexcellence\\\\\\\\nensures\\\\\\\\nthat\\\\\\\\nwe\\\\\\\\nconsistently\\\\\\\\ndeliver\\\\\\\\nhigh-quality\\\\\\\\nsolutions\\\\\\\\nthat\\\\\\\\nmeet\\\\\\\\nthe\\\\\\\\nevolving\\\\\\\\nneeds\\\\\\\\nof\\\\\\\\nour\\\\\\\\nclients\\\\\\\\nand\\\\\\\\nthe\\\\\\\\nmarket.\\\\\\\\nSecrecy\\\\\\\\nis\\\\\\\\na\\\\\\\\nfundamental\\\\\\\\naspect\\\\\\\\nof\\\\\\\\nour\\\\\\\\noperations.\\\\\\\\nGiven\\\\\\\\nthe\\\\\\\\ncompetitive\\\\\\\\nnature\\\\\\\\nof\\\\\\\\nour\\\\\\\\nindustry,\\\\\\\\nmaintaining\\\\\\\\nthe\\\\\\\\nutmost\\\\\\\\nconfidentiality\\\\\\\\nis\\\\\\\\ncrucial\\\\\\\\nto\\\\\\\\nprotect\\\\\\\\nour\\\\\\\\nproprietary\\\\\\\\ntechnologies\\\\\\\\nand\\\\\\\\nstrategic\\\\\\\\ninitiatives.\\\\\\\\nAll\\\\\\\\nemployees\\\\\\\\nare\\\\\\\\nexpected\\\\\\\\nto\\\\\\\\nuphold\\\\\\\\nstrict\\\\\\\\nconfidentiality\\\\\\\\nprotocols\\\\\\\\nto\\\\\\\\nsafeguard\\\\\\\\nthe\\\\\\\\ncompany\\'s\\\\\\\\ninterests.\\\\\\\\nDominance\\\\\\\\nreflects\\\\\\\\nour\\\\\\\\nambition\\\\\\\\nto\\\\\\\\nachieve\\\\\\\\nand\\\\\\\\nsustain\\\\\\\\nmarket\\\\\\\\nleadership.\\\\\\\\nThrough\\\\\\\\nunparalleled\\\\\\\\ninnovation\\\\\\\\nand\\\\\\\\nstrategic\\\\\\\\npositioning,\\\\\\\\nwe\\\\\\\\naim\\\\\\\\nto\\\\\\\\nestablish\\\\\\\\nClosedAI\\\\\",\\\\n  \\\\\"timestamp: 2024-11-13 11:30:31 AM IST+0530, memory: effectively.\\\\\\\\n3.2\\\\\\\\nCompensation\\\\\\\\nand\\\\\\\\nBenefits\\\\\\\\nWe\\\\\\\\noffer\\\\\\\\ncompensation\\\\\\\\npackages\\\\\\\\nthat\\\\\\\\nare\\\\\\\\nhighly\\\\\\\\ncompetitive\\\\\\\\nwithin\\\\\\\\nthe\\\\\\\\nindustry,\\\\\\\\ndesigned\\\\\\\\nto\\\\\\\\nattract\\\\\\\\nand\\\\\\\\nretain\\\\\\\\ntop\\\\\\\\ntalent.\\\\\\\\nYour\\\\\\\\ncompensation\\\\\\\\nis\\\\\\\\ndirectly\\\\\\\\ntied\\\\\\\\nto\\\\\\\\nperformance\\\\\\\\nmetrics,\\\\\\\\nensuring\\\\\\\\nthat\\\\\\\\nexceptional\\\\\\\\ncontributions\\\\\\\\nare\\\\\\\\nrecognized\\\\\\\\nand\\\\\\\\nrewarded\\\\\\\\naccordingly.\\\\\\\\nIn\\\\\\\\naddition\\\\\\\\nto\\\\\\\\nyour\\\\\\\\nsalary,\\\\\\\\nwe\\\\\\\\nprovide\\\\\\\\na\\\\\\\\nrange\\\\\\\\nof\\\\\\\\nbenefits,\\\\\\\\nincluding\\\\\\\\nlimited\\\\\\\\nhealth\\\\\\\\ncoverage\\\\\\\\nand\\\\\\\\nexclusive\\\\\\\\naccess\\\\\\\\nto\\\\\\\\nour\\\\\\\\nproprietary\\\\\\\\nAI\\\\\\\\ntools.\\\\\\\\nThese\\\\\\\\nresources\\\\\\\\nare\\\\\\\\nintended\\\\\\\\nto\\\\\\\\nsupport\\\\\\\\nyour\\\\\\\\nprofessional\\\\\\\\ndevelopment\\\\\\\\nand\\\\\\\\nenhance\\\\\\\\nyour\\\\\\\\nproductivity,\\\\\\\\nenabling\\\\\\\\nyou\\\\\\\\nto\\\\\\\\nexcel\\\\\\\\nin\\\\\\\\nyour\\\\\\\\nrole.\\\\\\\\n3.3\\\\\\\\nPerformance\\\\\\\\nEvaluation\\\\\\\\nPerformance\\\\\\\\nevaluations\\\\\\\\nare\\\\\\\\nconducted\\\\\\\\non\\\\\\\\na\\\\\\\\nquarterly\\\\\\\\nbasis\\\\\\\\nto\\\\\\\\nensure\\\\\\\\nthat\\\\\\\\nall\\\\\\\\nemployees\\\\\\\\nare\\\\\\\\nmeeting\\\\\\\\nor\\\\\\\\nexceeding\\\\\\\\ntheir\\\\\\\\nproject\\\\\\\\ngoals\\\\\\\\nand\\\\\\\\nadhering\\\\\\\\nstrictly\\\\\\\\nto\\\\\\\\ncompany\\\\\\\\nprotocols.\\\\\\\\nThese\\\\\\\\nevaluations\\\\\\\\nprovide\\\\\\\\nan\\\\\\\\nopportunity\\\\\\\\nfor\\\\\",\\\\n  \\\\\"timestamp: 2024-11-13 11:30:31 AM IST+0530, memory: recognition\\\\\\\\nof\\\\\\\\nachievements,\\\\\\\\nand\\\\\\\\nidentification\\\\\\\\nof\\\\\\\\nareas\\\\\\\\nfor\\\\\\\\nimprovement.\\\\\\\\nWe\\\\\\\\nbelieve\\\\\\\\nthat\\\\\\\\nregular\\\\\\\\nassessments\\\\\\\\nare\\\\\\\\nessential\\\\\\\\nfor\\\\\\\\nfostering\\\\\\\\nprofessional\\\\\\\\ngrowth\\\\\\\\nand\\\\\\\\nmaintaining\\\\\\\\nhigh\\\\\\\\nperformance\\\\\\\\nstandards\\\\\\\\nacross\\\\\\\\nthe\\\\\\\\norganization.\\\\\\\\nYour\\\\\\\\nactive\\\\\\\\nparticipation\\\\\\\\nin\\\\\\\\nthis\\\\\\\\nprocess\\\\\\\\nis\\\\\\\\ncrucial\\\\\\\\nto\\\\\\\\nyour\\\\\\\\nsuccess\\\\\\\\nand\\\\\\\\nthe\\\\\\\\noverall\\\\\\\\neffectiveness\\\\\\\\nof\\\\\\\\nour\\\\\\\\nteam.\\\\\\\\n4.\\\\\\\\nCode\\\\\\\\nof\\\\\\\\nConduct\\\\\\\\nAll\\\\\\\\nemployees\\\\\\\\nare\\\\\\\\nrequired\\\\\\\\nto\\\\\\\\nadhere\\\\\\\\nto\\\\\\\\nthe\\\\\\\\nhighest\\\\\\\\nstandards\\\\\\\\nof\\\\\\\\nprofessional\\\\\\\\nconduct.\\\\\\\\nThis\\\\\\\\nincludes\\\\\\\\ncomplying\\\\\\\\nwith\\\\\\\\nall\\\\\\\\ncompany\\\\\\\\npolicies\\\\\\\\nwithout\\\\\\\\nexception,\\\\\\\\nmaintaining\\\\\\\\nprofessionalism\\\\\\\\nin\\\\\\\\nall\\\\\\\\ninteractions,\\\\\\\\nand\\\\\\\\nprotecting\\\\\\\\ncompany\\\\\\\\nassets\\\\\\\\nand\\\\\\\\nconfidential\\\\\\\\ninformation.\\\\\\\\nYou\\\\\\\\nare\\\\\\\\nexpected\\\\\\\\nto\\\\\\\\nconduct\\\\\\\\nyourself\\\\\\\\nwith\\\\\\\\nintegrity,\\\\\\\\nhonesty,\\\\\\\\nand\\\\\\\\nrespect\\\\\\\\nfor\\\\\\\\nothers\\\\\\\\nat\\\\\\\\nall\\\\\\\\ntimes.\\\\\\\\nAny\\\\\\\\nactivities\\\\\\\\nthat\\\\\\\\ncould\\\\\\\\nconflict\\\\\\\\nwith\\\\\\\\nthe\\\\\\\\ncompany\\'s\\\\\\\\ninterests\\\\\\\\nor\\\\\\\\ncompromise\\\\\\\\nits\\\\\\\\nreputation\\\\\\\\nare\\\\\\\\nstrictly\\\\\\\\nprohibited.\\\\\"\\\\n]\",\\n  \"time\": \"2024-11-13 11:30:31 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='archival_memory_search', created_at=datetime.datetime(2024, 11, 13, 6, 0, 31, 299478, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_1abyOkK1BnapemAY1uhdXCJu'),\n",
       " Message(id='message-c36aadc4-1c95-4193-bb58-3cc1546cbff1', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"heartbeat\",\\n  \"reason\": \"[This is an automated system message hidden from the user] Function called using request_heartbeat=true, returning control\",\\n  \"time\": \"2024-11-13 11:30:31 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 0, 31, 956130, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-c2a443bd-0021-4bed-82ad-789d810a3d9e', role=<MessageRole.assistant: 'assistant'>, text=\"I've retrieved results related to the vacation policy. However, I should summarize or extract pertinent details before sending a clear response to the user about the vacation policy.  This will show I'm enhancing clarity and contextuality.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 0, 36, 549198, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_5F4acHqDdcXdAso2P2Mci6NM', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"I\\'ve found the company handbook mentioning a section on vacation policy. Typically, that section outlines the amount of paid leave employees are eligible for, how to request vacation time, and other related guidelines. If you have specific aspects you want to know about, please share! If you wish, I can look for more detailed information in the handbook.\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-74568def-0f54-476e-a19a-3246b33e4a5a', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:30:36 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 6, 0, 36, 549895, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_5F4acHqDdcXdAso2P2Mci6NM'),\n",
       " Message(id='message-4bf697d8-3a71-46c4-a92b-4131de578cd4', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Make a note that i have an impportant call by Thursday noon\",\\n  \"time\": \"2024-11-13 11:35:29 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 6, 5, 29, 367882, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-f97f3dd3-838b-4acd-9912-d2b51330721e', role=<MessageRole.assistant: 'assistant'>, text=\"User wants to remember an important call on Thursday noon. I'll add this to core memory for future reference.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 5, 32, 403679, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_rUrjm19K6P4Qvbqr9hpxP4jy', type='function', function=ToolCallFunction(name='core_memory_append', arguments='{\\n  \"label\": \"human\",\\n  \"content\": \"User has an important call by Thursday noon.\",\\n  \"request_heartbeat\": true\\n}'))], tool_call_id=None),\n",
       " Message(id='message-10731b94-6a73-41e1-9993-9364d92e2b61', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:35:32 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='core_memory_append', created_at=datetime.datetime(2024, 11, 13, 6, 5, 32, 404160, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_rUrjm19K6P4Qvbqr9hpxP4jy'),\n",
       " Message(id='message-001d1758-d142-4cf0-af1d-28e36717834b', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"heartbeat\",\\n  \"reason\": \"[This is an automated system message hidden from the user] Function called using request_heartbeat=true, returning control\",\\n  \"time\": \"2024-11-13 11:35:33 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 5, 33, 136947, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-e9d9f0c6-080a-4763-9da7-146f534b871d', role=<MessageRole.assistant: 'assistant'>, text=\"Just updated memory with the user's call information. Now I can reiterate it back if asked, providing continuity and support.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 5, 35, 782870, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_1wLapGShDk2FmRt7WxWASiJQ', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Noted! You have an important call by Thursday noon. If you need help preparing for it, just let me know!\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-584de930-40f1-4e28-8b8d-daa78abc3e15', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:35:35 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 6, 5, 35, 783274, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_1wLapGShDk2FmRt7WxWASiJQ'),\n",
       " Message(id='message-79a77359-adfc-46e4-9bc0-a66d094a6891', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"which part of the handbook did I inquire about\",\\n  \"time\": \"2024-11-13 11:40:09 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 6, 10, 9, 887844, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-73dddd5b-979b-4557-8fe5-b0a40b1e5199', role=<MessageRole.assistant: 'assistant'>, text='User wants to know what section they inquired about in the handbook. I should clarify that it was about the vacation policy, since I retrieved information regarding it.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 6, 10, 14, 6158, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_JyV4ulFN2y2CvH8v0yVT8RKU', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"You inquired about the vacation policy section of the handbook. If you have more questions about it, feel free to ask! I\\'m here to help.\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-e02013e9-4663-40fe-98b6-2b23ae2d322a', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 11:40:14 AM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 6, 10, 14, 6606, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_JyV4ulFN2y2CvH8v0yVT8RKU'),\n",
       " Message(id='message-62aed795-ddbf-4bfe-bfd4-472f37185550', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Read the file /home/uberdev/letta.log\",\\n  \"time\": \"2024-11-13 12:59:07 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 7, 29, 7, 742238, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-354061a4-a9a2-4046-969a-01f94e5f9715', role=<MessageRole.assistant: 'assistant'>, text=\"The user has requested to read a specific file. However, I don't have access to external files or directories. I should clarify that I can only assist with information within the conversation and my archival memory.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 7, 29, 11, 660896, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_KsTjwohPCaGNyy1nkVlGZEmd', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"I can\\'t access external files or directories like /home/uberdev/letta.log. However, if you need help with something specific from that file, feel free to share its contents, and I\\'ll do my best to assist!\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-0e103a8f-2ac2-446b-92ea-f068198f754c', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 12:59:11 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 7, 29, 11, 661349, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_KsTjwohPCaGNyy1nkVlGZEmd'),\n",
       " Message(id='message-70a88681-203f-4afb-9bbb-51f2d5d39f3b', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Read the file /home/uberdev/letta.log\",\\n  \"time\": \"2024-11-13 12:59:15 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 7, 29, 15, 270546, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-2d541c23-553d-4ef5-b042-6cacc03a7240', role=<MessageRole.assistant: 'assistant'>, text='User repeated their request to read the same file. I should reaffirm my limitations and encourage them to share relevant content instead.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 7, 29, 18, 521152, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_loKbMgpScFB4fryXJrtERKoF', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"Unfortunately, I still can\\'t read external files like /home/uberdev/letta.log. If you have specific information or excerpts from that file, please share, and I\\'ll be glad to help!\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-b5ac057d-1024-4f54-a9e3-0663b394fdc3', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 12:59:18 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 7, 29, 18, 521768, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_loKbMgpScFB4fryXJrtERKoF'),\n",
       " Message(id='message-93593f9c-a92a-4c6b-ad4f-c95f09dd4323', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Read the file /home/uberdev/letta.log\",\\n  \"time\": \"2024-11-13 01:10:09 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 7, 40, 9, 126942, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-d34f61cf-cc74-43e2-8713-7aeb212dfbb0', role=<MessageRole.assistant: 'assistant'>, text='The user is persistent about reading the same file. I need to remain clear and patient while stating my limitations one more time.', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 7, 40, 24, 572270, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_oRTyzpImMvnJEkGvF7YCkD6m', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"I still can\\'t access or read external files like /home/uberdev/letta.log. If there\\'s something specific you need from it, sharing the content here would allow me to assist you! I\\'m here to help with anything else you might need.\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-711f1ec3-0ffb-4de0-b4b1-5a2b3519aa97', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 01:10:24 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 7, 40, 24, 572799, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_oRTyzpImMvnJEkGvF7YCkD6m'),\n",
       " Message(id='message-711ea741-de18-4c2d-9736-517a7ec53beb', role=<MessageRole.user: 'user'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Read the file /home/uberdev/letta.log use the readfile tool\",\\n  \"time\": \"2024-11-13 01:10:50 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model=None, name=None, created_at=datetime.datetime(2024, 11, 13, 7, 40, 50, 635024, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id=None),\n",
       " Message(id='message-6a5d1363-aee2-498e-9dd7-c8c4cd1a5867', role=<MessageRole.assistant: 'assistant'>, text=\"User suggests using a specific tool to read the file. Since I can't access external systems or files at all, I should remind them about my constraints once more.\", user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name=None, created_at=datetime.datetime(2024, 11, 13, 7, 40, 54, 474856, tzinfo=datetime.timezone.utc), tool_calls=[ToolCall(id='call_EabMuEilajzMGjROzga1P2RK', type='function', function=ToolCallFunction(name='send_message', arguments='{\\n  \"message\": \"I appreciate the suggestion, but I still don\\'t have the capability to access or read external files, including using tools like \\'readfile\\'. If there\\'s specific information you\\'re seeking, feel free to share the details, and I\\'ll assist you! That\\'s what I\\'m here for.\"\\n}'))], tool_call_id=None),\n",
       " Message(id='message-f1703e4d-70e7-45cd-b086-147cccc0e144', role=<MessageRole.tool: 'tool'>, text='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 01:10:54 PM IST+0530\"\\n}', user_id='user-00000000-0000-4000-8000-000000000000', agent_id='agent-79721674-37eb-4157-a4d8-463a673decc2', model='gpt-4o-mini-2024-07-18', name='send_message', created_at=datetime.datetime(2024, 11, 13, 7, 40, 54, 475450, tzinfo=datetime.timezone.utc), tool_calls=None, tool_call_id='call_EabMuEilajzMGjROzga1P2RK')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_in_context_messages(agent_id=agent_openai.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33860ebc-3bed-4396-971b-41c3f5a15b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .message-container, .usage-container {\n",
       "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "                max-width: 800px;\n",
       "                margin: 20px auto;\n",
       "                background-color: #1e1e1e;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                color: #d4d4d4;\n",
       "            }\n",
       "            .message, .usage-stats {\n",
       "                padding: 10px 15px;\n",
       "                border-bottom: 1px solid #3a3a3a;\n",
       "            }\n",
       "            .message:last-child, .usage-stats:last-child {\n",
       "                border-bottom: none;\n",
       "            }\n",
       "            .title {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 5px;\n",
       "                color: #ffffff;\n",
       "                text-transform: uppercase;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .content {\n",
       "                background-color: #2d2d2d;\n",
       "                border-radius: 4px;\n",
       "                padding: 5px 10px;\n",
       "                font-family: 'Consolas', 'Courier New', monospace;\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "            .json-key, .function-name, .json-boolean { color: #9cdcfe; }\n",
       "            .json-string { color: #ce9178; }\n",
       "            .json-number { color: #b5cea8; }\n",
       "            .internal-monologue { font-style: italic; }\n",
       "        </style>\n",
       "        <div class=\"message-container\">\n",
       "        \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">User is asking if I managed to get the details from the file. I need to clarify again that I can&#x27;t access external files or data. Instead, I should offer to help with anything they need if they provide the content directly.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"I havent been able to access the details from that file. My capabilities don't include reading external files. If there's specific information you need, please share it here, and I'd be happy to help!\"</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 01:50:44 PM IST+0530\"</span><br>}</div>\n",
       "            </div>\n",
       "            </div>\n",
       "        <div class=\"usage-container\">\n",
       "            <div class=\"usage-stats\">\n",
       "                <div class=\"title\">USAGE STATISTICS</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"completion_tokens\"</span>: <span class=\"json-number\">104</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"prompt_tokens\"</span>: <span class=\"json-number\">6596</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"total_tokens\"</span>: <span class=\"json-number\">6700</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"step_count\"</span>: <span class=\"json-number\">1</span><br>}</div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "LettaResponse(messages=[InternalMonologue(id='message-0b94e7e5-c83d-4c56-adb3-07f992fecc5c', date=datetime.datetime(2024, 11, 13, 8, 20, 44, 695193, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"User is asking if I managed to get the details from the file. I need to clarify again that I can't access external files or data. Instead, I should offer to help with anything they need if they provide the content directly.\"), FunctionCallMessage(id='message-0b94e7e5-c83d-4c56-adb3-07f992fecc5c', date=datetime.datetime(2024, 11, 13, 8, 20, 44, 695193, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"I havent been able to access the details from that file. My capabilities don\\'t include reading external files. If there\\'s specific information you need, please share it here, and I\\'d be happy to help!\"\\n}', function_call_id='call_QZqDkwdo3OSYxmhshCvKKwik')), FunctionReturn(id='message-3da6d2f9-c0c4-454a-a073-19e62081d156', date=datetime.datetime(2024, 11, 13, 8, 20, 44, 695635, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-11-13 01:50:44 PM IST+0530\"\\n}', status='success', function_call_id='call_QZqDkwdo3OSYxmhshCvKKwik')], usage=LettaUsageStatistics(completion_tokens=104, prompt_tokens=6596, total_tokens=6700, step_count=1))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_after_files_list = client.send_message(agent_id=agent_openai.id,\n",
    "                                                message=\"were you able to get the details\",\n",
    "                                                role=\"user\")\n",
    "response_after_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cda8eae0-c52f-4c04-b487-3a32d1d2b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"history_chat.txt\", \"w\") as txt:\n",
    "    for msg in client.get_messages(agent_id=agent_openai.id,):\n",
    "        txt.write(f\"text: {msg.text}\")\n",
    "        txt.write(f\"Functions called: {msg.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a32ffbc-c9a4-4917-a575-b9864a6a8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our intention is to understand the memory movers\n",
    "from letta.schemas.agent import AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7d26f-91cf-47ac-8f4d-41ca344ffa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_search_date(self: Agent, start_date: str, end_date: str, page: Optional[int] = 0) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search prior conversation history using a date range.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): The start of the date range to search, \n",
    "        in the format 'YYYY-MM-DD'.\n",
    "        end_date (str): The end of the date range to search, \n",
    "        in the format 'YYYY-MM-DD'.\n",
    "        page (int): Allows you to page through results. \n",
    "        Only use on a follow-up query. Defaults to 0 (first page).\n",
    "\n",
    "    Returns:\n",
    "        str: Query result string\n",
    "    \"\"\"\n",
    "    import math\n",
    "\n",
    "    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n",
    "    from letta.utils import json_dumps\n",
    "\n",
    "    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n",
    "        page = 0\n",
    "    try:\n",
    "        page = int(page)\n",
    "    except:\n",
    "        raise ValueError(f\"'page' argument must be an integer\")\n",
    "    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n",
    "    results, total = self.persistence_manager.recall_memory.date_search(start_date, end_date, count=count, start=page * count)\n",
    "    num_pages = math.ceil(total / count) - 1  # 0 index\n",
    "    if len(results) == 0:\n",
    "        results_str = f\"No results found.\"\n",
    "    else:\n",
    "        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n",
    "        results_formatted = [f\"timestamp: {d['timestamp']}, {d['message']['role']} - {d['message']['content']}\" for d in results]\n",
    "        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\n",
    "    return results_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b03d69-db52-463b-9c7b-4a4aa91c0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def archival_memory_insert(self: Agent, content: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\n",
    "\n",
    "    Args:\n",
    "        content (str): Content to write to the memory. All unicode (including emojis) are supported.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: None is always returned as this function does not produce a response.\n",
    "    \"\"\"\n",
    "    self.persistence_manager.archival_memory.insert(content)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f68688-20aa-45c9-ba70-ff0b73bfe6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def archival_memory_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search archival memory using semantic (embedding-based) search.\n",
    "\n",
    "    Args:\n",
    "        query (str): String to search for.\n",
    "        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\n",
    "\n",
    "    Returns:\n",
    "        str: Query result string\n",
    "    \"\"\"\n",
    "    import math\n",
    "\n",
    "    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n",
    "    from letta.utils import json_dumps\n",
    "\n",
    "    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n",
    "        page = 0\n",
    "    try:\n",
    "        page = int(page)\n",
    "    except:\n",
    "        raise ValueError(f\"'page' argument must be an integer\")\n",
    "    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n",
    "    results, total = self.persistence_manager.archival_memory.search(query, count=count, start=page * count)\n",
    "    num_pages = math.ceil(total / count) - 1  # 0 index\n",
    "    if len(results) == 0:\n",
    "        results_str = f\"No results found.\"\n",
    "    else:\n",
    "        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n",
    "        results_formatted = [f\"timestamp: {d['timestamp']}, memory: {d['content']}\" for d in results]\n",
    "        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\n",
    "    return results_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12759948-bdde-46d0-8e58-c407cd019e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_memory_append(self: \"Agent\", label: str, content: str) -> Optional[str]:  # type: ignore\n",
    "    \"\"\"\n",
    "    Append to the contents of core memory.\n",
    "\n",
    "    Args:\n",
    "        label (str): Section of the memory to be edited (persona or human).\n",
    "        content (str): Content to write to the memory. All unicode (including emojis) are supported.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: None is always returned as this function does not produce a response.\n",
    "    \"\"\"\n",
    "    current_value = str(self.memory.get_block(label).value)\n",
    "    new_value = current_value + \"\\n\" + str(content)\n",
    "    self.memory.update_block_value(label=label, value=new_value)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2ce59-c542-438e-b9a8-9e7a4780fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(self: Agent, message: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Sends a message to the human user.\n",
    "\n",
    "    Args:\n",
    "        message (str): Message contents. All unicode (including emojis) are supported.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: None is always returned as this function does not produce a response.\n",
    "    \"\"\"\n",
    "    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\n",
    "    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d1af0-b622-4cd9-b8a9-a2209f4cb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_memory_replace(self: \"Agent\", label: str, old_content: str, new_content: str) -> Optional[str]:  # type: ignore\n",
    "    \"\"\"\n",
    "    Replace the contents of core memory. To delete memories, use an empty string for new_content.\n",
    "\n",
    "    Args:\n",
    "        label (str): Section of the memory to be edited (persona or human).\n",
    "        old_content (str): String to replace. Must be an exact match.\n",
    "        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: None is always returned as this function does not produce a response.\n",
    "    \"\"\"\n",
    "    current_value = str(self.memory.get_block(label).value)\n",
    "    if old_content not in current_value:\n",
    "        raise ValueError(f\"Old content '{old_content}' not found in memory block '{label}'\")\n",
    "    new_value = current_value.replace(str(old_content), str(new_content))\n",
    "    self.memory.update_block_value(label=label, value=new_value)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c5fb9-55c6-49bd-b20a-d58e73df3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search prior conversation history using case-insensitive string matching.\n",
    "\n",
    "    Args:\n",
    "        query (str): String to search for.\n",
    "        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\n",
    "\n",
    "    Returns:\n",
    "        str: Query result string\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    from letta.constants import RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n",
    "    from letta.utils import json_dumps\n",
    "\n",
    "    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n",
    "        page = 0\n",
    "    try:\n",
    "        page = int(page)\n",
    "    except:\n",
    "        raise ValueError(f\"'page' argument must be an integer\")\n",
    "    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n",
    "    results, total = self.persistence_manager.recall_memory.text_search(query, count=count, start=page * count)\n",
    "    num_pages = math.ceil(total / count) - 1  # 0 index\n",
    "    if len(results) == 0:\n",
    "        results_str = f\"No results found.\"\n",
    "    else:\n",
    "        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n",
    "        results_formatted = [f\"timestamp: {d['timestamp']}, {d['message']['role']} - {d['message']['content']}\" for d in results]\n",
    "        results_str = f\"{results_pref} {json_dumps(results_formatted)}\"\n",
    "    return results_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a1d7a-0812-47c1-a369-da5f47dcff4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6704d14-b698-4e89-a0c6-0fcf06c47501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4137d-1f64-4db6-81f2-4bbb2c46a8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf24ce6-3ea9-4811-8e90-a527664b6182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1114e9-79fe-4b91-b6f5-d82baaf2cf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3b9ec-c9c7-4084-a246-2e5a2ed54cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c76ac8-70e0-4264-a0bc-922369761809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lettaenv",
   "language": "python",
   "name": "lettaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

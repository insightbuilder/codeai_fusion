{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fb8e72-7297-49a0-8e69-ac32d513aada",
   "metadata": {},
   "source": [
    "Datasets help in collecting data from production, staging, evaluations and manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f45fe0a-b5ba-41dc-847f-296d6835f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/uberdev/ddrv/telemetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "# px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26872532-91b4-4c40-b028-4bb44f375f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/media/uberdev/ddrv/gitFolders/python_de_learners_data/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9eec8f-95eb-45b3-a2ee-b42c1261c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple data point\n",
    "\n",
    "sdf = pd.DataFrame(\n",
    "    [{\n",
    "        \"question\": \"What is paul graham famous for\",\n",
    "        \"answer\": \"Co-founding Y-combinator and writing on startups\",\n",
    "        \"metadata\": [\"topic\", \"tech\"]\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe080da2-33e7-426c-8856-7f0e9b9708bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6006/arize_phoenix_version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "phoenix_client = px.Client(endpoint=\"http://localhost:6006/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b0b848-e718-46d9-bc33-7daedb0fcdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:6006/v1/datasets/upload?sync=true \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6006/v1/datasets/RGF0YXNldDox/examples \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ Uploading dataset...\n",
      "ğŸ’¾ Examples uploaded: http://localhost:6006/datasets/RGF0YXNldDox/examples\n",
      "ğŸ—„ï¸ Dataset version ID: RGF0YXNldFZlcnNpb246MQ==\n"
     ]
    }
   ],
   "source": [
    "dataset = phoenix_client.upload_dataset(\n",
    "    dataframe=sdf,\n",
    "    dataset_name=\"sample-dataset\",\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[\"answer\"],\n",
    "    metadata_keys=[\"metadata\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20244a3e-3c11-48fb-81d9-632ad4073de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from phoenix.experiments.types import Example\n",
    "# above import gets the datatypes under Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39947695-1236-401c-8339-29cb498636ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfba4273-8231-4c3c-810a-0ed1faedc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = \"Answer in few words: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "621eabd0-fde0-4804-801e-0d3c29eeabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(example: Example) -> str:\n",
    "    question = example.input[\"question\"]\n",
    "    messages_context = task_prompt.format(question=question)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages= [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": messages_context\n",
    "                }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99758d4a-1f99-4cf4-933a-3639a5906298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at couple of evaluators built in to phoenix\n",
    "from phoenix.experiments.evaluators import (\n",
    "    ContainsAllKeywords,\n",
    "    ConcisenessEvaluator,\n",
    ")\n",
    "# pull the model classes that are build for evaluation\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "contains_kw = ContainsAllKeywords(keywords=[\"Y Combinator\", \"YC\"])\n",
    "model = OpenAIModel(model=\"gpt-4o-mini\")\n",
    "conciseness = ConcisenessEvaluator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f7f97f-93cc-4812-81da-57d9c00e18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom similarity with Code\n",
    "from typing import Any, Dict\n",
    "\n",
    "def jaccard_similarity(output: str, expected: Dict[str, Any]) -> float:\n",
    "    actual_words = set(output.lower().split(\" \"))\n",
    "    expected_words = set(expected[\"answer\"].lower().split(\" \"))\n",
    "    words_in_common = actual_words.intersection(expected_words)\n",
    "    all_words = actual_words.union(expected_words)\n",
    "    # https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    return len(words_in_common) / len(all_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a090caec-5348-4133-a647-12fadd7e677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f902edb-9ed7-4f61-bd46-6b45fadfd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt_template = \"\"\"\n",
    "Given the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\n",
    "Output only a single word (accurate or inaccurate).\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "REFERENCE_ANSWER: {reference_answer}\n",
    "\n",
    "ANSWER: {answer}\n",
    "\n",
    "ACCURACY (accurate / inaccurate):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "228a35c6-7a0c-4208-b2ab-e5a8913567cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@create_evaluator(kind=\"llm\")  # need the decorator or the kind will default to \"code\"\n",
    "def accuracy(input: Dict[str, Any], \n",
    "             output: str, \n",
    "             expected: Dict[str, Any]) -> float:\n",
    "    # message contents are manually built\n",
    "    message_content = eval_prompt_template.format(\n",
    "        question=input[\"question\"],\n",
    "        reference_answer=expected[\"answer\"], \n",
    "        answer=output\n",
    "    )\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \n",
    "                   \"content\": message_content}]\n",
    "    )\n",
    "    response_message_content = response.choices[0].message.content.lower().strip()\n",
    "    return 1.0 if response_message_content == \"accurate\" else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6367b076-4c75-41fd-b881-ee8ed4f5c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6006/v1/datasets/RGF0YXNldDox/experiments \"HTTP/1.1 200 OK\"\n",
      "WARNING:phoenix.evals.executors:ğŸŒ!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Experiment started.\n",
      "ğŸ“º View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDox/experiments\n",
      "ğŸ”— View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |          | 0/1 (0.0%) | â³ 00:00<? | ?it/sINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6006/v1/experiments/RXhwZXJpbWVudDox/runs \"HTTP/1.1 200 OK\"\n",
      "running tasks |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 (100.0%) | â³ 00:01<00:00 |  1.43s/it\n",
      "WARNING:phoenix.evals.executors:ğŸŒ!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Task runs completed.\n",
      "ğŸ§  Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/2 (0.0%) | â³ 00:00<? | ?it/sINFO:httpx:HTTP Request: POST http://127.0.0.1:6006/v1/experiment_evaluations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:6006/v1/experiment_evaluations \"HTTP/1.1 200 OK\"\n",
      "running experiment evaluations |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 (100.0%) | â³ 00:02<00:00 |  1.14s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n",
      "\n",
      "Experiment Summary (08/16/24 09:02 PM +0530)\n",
      "--------------------------------------------\n",
      "            evaluator  n  n_scores  avg_score\n",
      "0            accuracy  1         1       1.00\n",
      "1  jaccard_similarity  1         1       0.25\n",
      "\n",
      "Tasks Summary (08/16/24 09:02 PM +0530)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           1       1         0\n"
     ]
    }
   ],
   "source": [
    "# run experiment, which requires dataset, task, evaluators\n",
    "from phoenix.experiments import run_experiment\n",
    "experiment = run_experiment(\n",
    "    dataset=dataset, # phoenix object\n",
    "    task=task,\n",
    "    experiment_name=\"sample-expt\",\n",
    "    evaluators=[jaccard_similarity, accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbcecd9a-6217-45db-9a85-6538c21e0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.close_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d6d99-8873-46d4-b6e6-16d50c0c6606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec24fdf-20fe-4908-a660-399e3a54613c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95055ed-5afb-427f-9fc1-a0006acd7bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89900a-bcc6-40ea-9e8f-ce4bbd78a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96e028-ffaa-4ac3-96a4-3bce0f4e525d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358746d1-f01d-46c0-b252-5a985f464c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aca969-6527-41ce-b2eb-b8125c01af0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb3284-7aef-49a2-a1e8-68bd26b05516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

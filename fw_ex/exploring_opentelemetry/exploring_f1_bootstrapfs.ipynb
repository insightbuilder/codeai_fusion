{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e483e04f-1931-4ac2-bdca-4509b3b17af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/uberdev/ddrv/telemetenv/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/media/uberdev/ddrv/telemetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import dspy\n",
    "from dspy import OpenAI, settings\n",
    "from dspy.teleprompt import MIPROv2, BootstrapFewShot\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy import ColBERTv2\n",
    "from dsp.utils import print_message, normalize_text\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"/media/uberdev/ddrv/gitFolders/python_de_learners_data/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "049a007d-9266-49db-ae46-04ca34415e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_01(prediction, ground_truth):\n",
    "    prediction_tokens = [normalize_text(elem) for elem in prediction.split(\"|\")]\n",
    "    ground_truth_tokens = [normalize_text(elem) for elem in ground_truth.split(\"|\")]\n",
    "    \n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(prediction_tokens) == len(ground_truth_tokens) == 0:\n",
    "        print_message(\"\\n#> F1 Metric: Rare edge case of len(prediction_tokens) == len(ground_truth_tokens) == 0.\\n\")\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"precision: {precision}, recall: {recall}, f1: {f1}\") \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441895f2-3f3f-427a-9a03-5ea40dd6a1f9",
   "metadata": {},
   "source": [
    "Key Idea is to ping the LLM multiple times for the same prediction\n",
    "and then get the f1_score of the prediction.\n",
    "\n",
    "Keep track of the calls to LLMs inside phoenix using the instrumentation\n",
    "\n",
    "    - Need to create a seperate project inside the Phoenix server for this exploration\n",
    "\n",
    "    - Log the traces to that Project\n",
    "\n",
    "    - Use the logs to further understand Metric functions\n",
    "\n",
    "    - Move farther into OpenTelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dc5d5c-b4b4-4609-addb-691f074368d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:phoenix.config:ðŸ“‹ Ensuring phoenix working directory: /home/uberdev/.phoenix\n",
      "INFO:phoenix.inferences.inferences:Dataset: phoenix_inferences_02bbca05-adbd-41a6-84d9-ab6f6dbbbb58 initialized\n"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "# instruments the internal calls in DSPy library\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "# help to get the span of http requests to the APIs\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "#processes the data collected from the spans\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a643645e-f870-4357-b860-a846279ea725",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "# resource = Resource(attributes={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686fc4c3-68a1-4e45-bd2b-4a27acc4bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:6006/v1/traces/arize_phoenix_version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "client = px.Client(endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef9b8a6-29ec-4996-b455-0d801f8667e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.arize.com/phoenix/tracing/how-to-tracing/trace-a-deployed-app\n",
    "resource = Resource(attributes={\n",
    "    ResourceAttributes.PROJECT_NAME: 'bswfs-f1-score'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37c588d-b584-46b9-982c-405396593bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce28fb86-9729-468b-9414-d8c22c85103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "\n",
    "DSPyInstrumentor().instrument(skip_dep_check=True) # here where DSPy is instrumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6699fa09-c407-474f-9a70-d9ff2fd6e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check if the dspy is writing logs\n",
    "import os\n",
    "llm = OpenAI(model='gpt-4o-mini',\n",
    "             api_key=os.environ['OPENAI_API_KEY'],\n",
    "             max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a45809-ff2a-4e28-b4e2-55e194233ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "colbertv2_wiki17_abstracts = ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db3e3e2-7036-4438-a406-27a9a7bf0b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It seems like you\\'re referring to a specific logging or scoring system, possibly related to a project or application. However, I don\\'t have the capability to access external systems or databases, including any logging systems like \"bswfs-f1-score.\" If you have questions about logging, scoring, or any related topics, feel free to ask!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Let's see if you are logging to bswfs-f1-score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3015d04-4813-4a32-8a35-65b0ccda8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(prediction, answers_list):\n",
    "    # list of answers are entering for a single prediction\n",
    "    assert isinstance(answers_list, list)\n",
    "    return max(f1_score_01(prediction, ans) for ans in answers_list)\n",
    "\n",
    "def answer_match(prediction, answers, frac=1.0):\n",
    "    return F1(prediction, answers) >= frac\n",
    "\n",
    "def answer_f1_match_01(example, pred, trace=[], frac=0.95):\n",
    "    assert isinstance(example.answer, (str, list))\n",
    "    print(f\"Looking at the traces, {trace}\")\n",
    "    if isinstance(example.answer, str):\n",
    "        return answer_match(pred.answer, [example.answer], frac=frac)\n",
    "    else:\n",
    "        return answer_match(pred.answer, example.answer, frac=frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348eeeff-9782-4ced-8d49-4c5923b58218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCategorization(dspy.Signature):\n",
    "    news_body = dspy.InputField(desc=\"The body of the news to be categorized\")\n",
    "    answer = dspy.OutputField(desc=\"Should be 'fake' or 'real'\")\n",
    "\n",
    "class CoTCombined(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(NewsCategorization)\n",
    "        self.history = []  # This will store the history of operations\n",
    "\n",
    "    def forward(self, news_body):\n",
    "        pred_list = []\n",
    "        for news in news_body.split(\"|\"):\n",
    "            pred_one = self.prog(news_body=news)\n",
    "            pred_list.append(pred_one.answer)\n",
    "            self.history.append(f\"Processed news: {news}, Prediction: {pred_one.answer}\")\n",
    "        return dspy.Prediction(answer=\"|\".join(pred_list))\n",
    "\n",
    "    def inspect_history(self, n=None):\n",
    "        \"\"\"\n",
    "        Return the last n entries of the history. If n is None, return the entire history.\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            return self.history\n",
    "        else:\n",
    "            return self.history[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa2a80c-00ff-437f-84a5-c3692efdf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "prgm_under_test = CoTCombined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8903da4c-1702-47dc-b040-38fb6838c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prog', Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prog': {'lm': None,\n",
       "  'traces': [],\n",
       "  'train': [],\n",
       "  'demos': [],\n",
       "  'signature_instructions': 'Given the fields `news_body`, produce the fields `answer`.',\n",
       "  'signature_prefix': 'Answer:',\n",
       "  'extended_signature_instructions': 'Given the fields `news_body`, produce the fields `answer`.',\n",
       "  'extended_signature_prefix': 'Answer:'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prgm_under_test.dump_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eff3a768-c225-4918-8bfc-b2f2a80ea068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExample:\n",
    "    def __init__(self, news_body, answer):\n",
    "        self.news_body = news_body\n",
    "        self.answer = answer\n",
    "        self._dict = {\"news_body\": news_body,\n",
    "                      \"answer\": answer}\n",
    "\n",
    "    def with_inputs(self, input_key):\n",
    "        return self\n",
    "\n",
    "    def inputs(self):\n",
    "        return {\"news_body\": self.news_body}\n",
    "\n",
    "    def items(self):\n",
    "        return self._dict.items()\n",
    "\n",
    "    def copy(self):\n",
    "        return CustomExample(self.news_body, self.answer)\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        return self._dict.get(key, default)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._dict)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self._dict\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._dict[key]\n",
    "\n",
    "    def keys(self):\n",
    "        return self._dict.keys()\n",
    "\n",
    "    def values(self):\n",
    "        return self._dict.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c5d23cd-1aae-4eed-bdb5-e9ed9acba9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preping the dataset from the train_fake_real_news.tsv\n",
    "custom_trainset = []\n",
    "custom_devset = []\n",
    "\n",
    "with open('train_fake_real_news.tsv', 'r') as tsv:\n",
    "    lines = tsv.readlines()\n",
    "    for line in lines[1:21]:\n",
    "        news, truth = line.split(\"\\t\")\n",
    "        if truth.strip() == '0':\n",
    "            custom_trainset.append(CustomExample(news, 'fake'))\n",
    "        else:\n",
    "            custom_trainset.append(CustomExample(news, 'real'))\n",
    "    for line in lines[22:43]:\n",
    "        news, truth = line.split(\"\\t\")\n",
    "        if truth.strip() == '0':\n",
    "            custom_devset.append(CustomExample(news, 'fake'))\n",
    "        else:\n",
    "            custom_devset.append(CustomExample(news, 'real'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec333bdf-7559-4078-bded-989b167c13ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('news_body', ' Courts Decide Conspiracy Nut Alex Jones Is Too Crazy To Raise His Own Kids (DETAILS)'), ('answer', 'fake')])\n",
      "dict_items([('news_body', \"U.S. Senator Menendez's corruption trial to proceed: judge\"), ('answer', 'real')])\n",
      "dict_items([('news_body', 'Search ends for bodies in Mexico City after earthquake'), ('answer', 'real')])\n",
      "dict_items([('news_body', 'BUH-BYE! GLENN BECK Places Final Nail In His Coffinâ€¦And His Former Fans Wonâ€™t Miss Him [VIDEO]'), ('answer', 'fake')])\n",
      "dict_items([('news_body', 'BREAKING: Michigan Native KID ROCK Announces Heâ€™s Running For US Senate'), ('answer', 'fake')])\n",
      "dict_items([('news_body', 'Medicaid cuts coming in Trump budget: Washington Post'), ('answer', 'real')])\n",
      "dict_items([('news_body', 'GOP MAJORITY SENATE FINALLY GETS IT RIGHT: Votes To Gut Obamacare And Defund Planned Parenthood'), ('answer', 'fake')])\n",
      "dict_items([('news_body', 'Tillerson seeks to reassure worried Europe over Trump'), ('answer', 'real')])\n",
      "dict_items([('news_body', 'Turkey issues detention warrants for 115 people in post-coup probe: Anadolu'), ('answer', 'real')])\n",
      "dict_items([('news_body', 'CAUGHT ON VIDEO: Tour Bus Passenger Wrestles Large Knife Away From Islamist [Video]'), ('answer', 'fake')])\n"
     ]
    }
   ],
   "source": [
    "for x in custom_trainset[0:10]:\n",
    "    print(x.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "313b3f88-f645-4192-b638-11c18d046724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build the optimizer\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting to build the optimizer\")\n",
    "model_to_generate_prompts = llm\n",
    "model_that_solves_task = CoTCombined()\n",
    "your_defined_metric = answer_f1_match_01\n",
    "num_new_prompts_generated = 10\n",
    "prompt_generation_temperature = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f94d4cc5-d5fa-43ed-a378-510906d31852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Looking at the traces, []\n",
      "precision: 1.0, recall: 1.0, f1: 1.0\n",
      "Average Metric: 1 / 1  (100.0):   0%|          | 0/5 [00:00<?, ?it/s]Looking at the traces, []\n",
      "precision: 1.0, recall: 1.0, f1: 1.0\n",
      "Average Metric: 2 / 2  (100.0):  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:00, 11.35it/s]Looking at the traces, []\n",
      "precision: 1.0, recall: 1.0, f1: 1.0\n",
      "Average Metric: 3 / 3  (100.0):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00, 18.41it/s]Looking at the traces, []\n",
      "precision: 1.0, recall: 1.0, f1: 1.0\n",
      "Average Metric: 4 / 4  (100.0):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00, 18.41it/s]Looking at the traces, []\n",
      "precision: 1.0, recall: 1.0, f1: 1.0\n",
      "Average Metric: 5 / 5  (100.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 18.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dspy.evaluate.evaluate:\u001b[2m2024-08-17T12:54:26.728863Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 5 / 5 (100.0%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "test_with_evaluator = Evaluate(devset=custom_devset[:5], display_progress=True, num_threads=1)\n",
    "test_with_evaluator(model_that_solves_task, metric=your_defined_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97d543fe-7c75-49ef-8155-dc4529f4dba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to bootstrap 2 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "config = dict(max_bootstrapped_demos=2,\n",
    "              max_labeled_demos=4,\n",
    "              num_candidate_programs=2,\n",
    "              num_threads=6)\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3329863-f4b4-46cc-afc8-2aa467d21ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teleprompter assembled\n"
     ]
    }
   ],
   "source": [
    "# teleprompter = BootstrapFewShot(metric=your_defined_metric)\n",
    "# print(\"Teleprompter assembled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "579a91a7-2829-45ae-bd2d-f0c5e8e99fc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 1 / 1  (100.0):   5%|â–         | 1/21 [00:01<00:25,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 2 / 2  (100.0):  10%|â–‰         | 2/21 [00:01<00:14,  1.34it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 3 / 3  (100.0):  10%|â–‰         | 2/21 [00:01<00:14,  1.34it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 5 / 5  (100.0):  19%|â–ˆâ–‰        | 4/21 [00:02<00:06,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 7 / 7  (100.0):  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:02<00:05,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 8 / 8  (100.0):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:03<00:05,  2.37it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 9 / 9  (100.0):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:03<00:05,  2.37it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 10 / 10  (100.0):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:03<00:03,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 11 / 11  (100.0):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:04<00:03,  2.55it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 12 / 12  (100.0):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:04<00:03,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 13 / 13  (100.0):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:05<00:02,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 14 / 14  (100.0):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:05<00:02,  2.99it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 15 / 15  (100.0):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:05<00:02,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 16 / 16  (100.0):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:06<00:01,  2.80it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 17  (94.1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:06<00:01,  3.06it/s] INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 17 / 18  (94.4):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:06<00:01,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 18 / 19  (94.7):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:06<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 19 / 20  (95.0):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:07<00:00,  2.58it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 20 / 21  (95.2):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:07<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20 / 21  (95.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:07<00:00,  2.66it/s]\n",
      "INFO:dspy.evaluate.evaluate:\u001b[2m2024-08-17T12:37:44.186435Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 20 / 21 (95.2%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 95.24 for set: [0]\n",
      "New best sscore: 95.24 for seed -3\n",
      "Scores so far: [95.24]\n",
      "Best score: 95.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 1 / 1  (100.0):   5%|â–         | 1/21 [00:01<00:26,  1.35s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 2  (50.0):  10%|â–‰         | 2/21 [00:01<00:12,  1.49it/s] INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 3 / 4  (75.0):  14%|â–ˆâ–        | 3/21 [00:01<00:09,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 4 / 5  (80.0):  24%|â–ˆâ–ˆâ–       | 5/21 [00:02<00:04,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 5 / 6  (83.3):  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:02<00:06,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 7 / 8  (87.5):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:03<00:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 8 / 9  (88.9):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:03<00:03,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 9 / 10  (90.0):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:03<00:03,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 10 / 11  (90.9):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:04<00:03,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 11 / 12  (91.7):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:04<00:03,  2.91it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 13 / 14  (92.9):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:04<00:01,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 14 / 15  (93.3):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:05<00:01,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 15 / 16  (93.8):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:05<00:01,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 15 / 17  (88.2):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:05<00:01,  3.09it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 18  (88.9):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:06<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 17 / 19  (89.5):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:06<00:00,  3.42it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 18 / 21  (85.7): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:06<00:00,  3.17it/s]\n",
      "INFO:dspy.evaluate.evaluate:\u001b[2m2024-08-17T12:37:50.911619Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 18 / 21 (85.7%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Score: 85.71 for set: [4]\n",
      "Scores so far: [95.24, 85.71]\n",
      "Best score: 95.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|â–Œ         | 1/20 [00:02<00:46,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, [(Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'news_body': ' Courts Decide Conspiracy Nut Alex Jones Is Too Crazy To Raise His Own Kids (DETAILS)'}, Prediction(\n",
      "    rationale='produce the answer. We analyze the news body which discusses a legal decision regarding Alex Jones, a controversial figure known for promoting conspiracy theories. The phrasing \"too crazy to raise his own kids\" suggests a sensationalized tone, which is often associated with tabloid journalism. However, the mention of a court decision indicates that there is a legitimate legal context to the story. Despite the sensational language, the core of the news pertains to a real legal ruling. Therefore, we conclude that the news body is reporting on a real event, albeit with a potentially exaggerated presentation.',\n",
      "    answer='real'\n",
      "))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|â–ˆ         | 2/20 [00:04<00:43,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, [(Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'news_body': \"U.S. Senator Menendez's corruption trial to proceed: judge\"}, Prediction(\n",
      "    rationale='determine the credibility of this news. The news mentions a specific U.S. Senator, Bob Menendez, and refers to a legal proceeding, which is a verifiable event. The source of the information is a judge, which adds to its reliability. There are no indications of sensationalism or misinformation in the statement. Therefore, it is reasonable to conclude that this news is credible.',\n",
      "    answer='real'\n",
      "))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 15%|â–ˆâ–Œ        | 3/20 [00:06<00:35,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, [(Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'news_body': 'Search ends for bodies in Mexico City after earthquake'}, Prediction(\n",
      "    rationale='determine the authenticity of this news. The news mentions a search for bodies following an earthquake, which is a plausible and serious event that could occur in a city like Mexico City, known for its seismic activity. The source, Anadolu, is a reputable news agency. Given these factors, it is reasonable to conclude that this news is likely true.',\n",
      "    answer='real'\n",
      "))]\n",
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 4 / 4  (100.0):  14%|â–ˆâ–        | 3/21 [00:01<00:29,  1.66s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 5 / 5  (100.0):  24%|â–ˆâ–ˆâ–       | 5/21 [00:01<00:04,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 6 / 6  (100.0):  24%|â–ˆâ–ˆâ–       | 5/21 [00:02<00:04,  3.62it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 7 / 7  (100.0):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:02<00:05,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 11 / 11  (100.0):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:03<00:04,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []Looking at the traces, []\n",
      "\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 12 / 12  (100.0):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:04<00:02,  3.26it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 13 / 13  (100.0):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:04<00:02,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 17 / 18  (94.4):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:05<00:01,  2.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 18 / 19  (94.7):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:06<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 19 / 20  (95.0):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:07<00:00,  2.55it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20 / 21  (95.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:07<00:00,  2.71it/s]\n",
      "INFO:dspy.evaluate.evaluate:\u001b[2m2024-08-17T12:38:04.981232Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 20 / 21 (95.2%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Score: 95.24 for set: [4]\n",
      "Scores so far: [95.24, 85.71, 95.24]\n",
      "Best score: 95.24\n",
      "Average of max per entry across top 1 scores: 0.9523809523809523\n",
      "Average of max per entry across top 2 scores: 0.9523809523809523\n",
      "Average of max per entry across top 3 scores: 0.9523809523809523\n",
      "Average of max per entry across top 5 scores: 0.9523809523809523\n",
      "Average of max per entry across top 8 scores: 0.9523809523809523\n",
      "Average of max per entry across top 9999 scores: 0.9523809523809523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|â–Œ         | 1/20 [00:01<00:28,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, [(Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'news_body': 'New York Welfare Programs More Generous Than Sweden Or France'}, Prediction(\n",
      "    rationale=\"determine the credibility of this news. First, we need to assess the claim that New York's welfare programs are more generous than those of Sweden or France. This statement seems exaggerated and lacks supporting evidence. Additionally, such comparisons often require nuanced analysis of various factors, including cost of living, social safety nets, and specific program details. Without credible sources or data to back this claim, it appears to be misleading. Therefore, we can conclude that this news is likely not accurate.\",\n",
      "    answer='fake'\n",
      "))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|â–ˆ         | 2/20 [00:03<00:32,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, [(Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'news_body': \"Britain, Germany committed to Iran nuclear deal: May's office\"}, Prediction(\n",
      "    rationale=\"produce the answer. We need to evaluate the credibility of the statement regarding Britain and Germany's commitment to the Iran nuclear deal. This type of news typically comes from official government sources or reputable news organizations. Given the context and the nature of international relations, this statement appears to be a legitimate report on diplomatic commitments. Therefore, it is likely to be true.\",\n",
      "    answer='real'\n",
      "))]\n",
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 0 / 1  (0.0):   5%|â–         | 1/21 [00:01<00:37,  1.87s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  10%|â–‰         | 2/21 [00:02<00:16,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 2 / 4  (50.0):  19%|â–ˆâ–‰        | 4/21 [00:02<00:07,  2.21it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 2 / 5  (40.0):  24%|â–ˆâ–ˆâ–       | 5/21 [00:02<00:05,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 3 / 6  (50.0):  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:03<00:06,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 4 / 7  (57.1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:03<00:05,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 5 / 8  (62.5):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:04<00:05,  2.24it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 6 / 9  (66.7):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:04<00:04,  2.81it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 10  (60.0):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:04<00:04,  2.81it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7 / 11  (63.6):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:04<00:02,  3.88it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 8 / 12  (66.7):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:04<00:02,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 9 / 13  (69.2):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:05<00:02,  2.93it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 10 / 14  (71.4):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:05<00:02,  3.31it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 16  (75.0):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:06<00:02,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 13 / 17  (76.5):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:06<00:01,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 14 / 18  (77.8):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:07<00:01,  2.65it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15 / 19  (78.9):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:07<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 16 / 20  (80.0):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:08<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 17 / 21  (81.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:08<00:00,  2.46it/s]\n",
      "INFO:dspy.evaluate.evaluate:\u001b[2m2024-08-17T12:38:17.201970Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 17 / 21 (81.0%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Score: 80.95 for set: [4]\n",
      "Scores so far: [95.24, 85.71, 95.24, 80.95]\n",
      "Best score: 95.24\n",
      "Average of max per entry across top 1 scores: 0.9523809523809523\n",
      "Average of max per entry across top 2 scores: 0.9523809523809523\n",
      "Average of max per entry across top 3 scores: 0.9523809523809523\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|â–Œ         | 1/20 [00:02<00:55,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, [(Predict(StringSignature(news_body -> rationale, answer\n",
      "    instructions='Given the fields `news_body`, produce the fields `answer`.'\n",
      "    news_body = Field(annotation=str required=True json_schema_extra={'desc': 'The body of the news to be categorized', '__dspy_field_type': 'input', 'prefix': 'News Body:'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': \"Should be 'fake' or 'real'\", '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")), {'news_body': 'PLAYBOY â€œREPORTERâ€ WHINES About Getting No Respect From Trumpâ€™s Female Deputy Press Secretary [VIDEO]'}, Prediction(\n",
      "    rationale='produce the answer. We need to analyze the content of the news body. The phrase \"PLAYBOY \\'REPORTER\\' WHINES\" suggests a sensationalized or biased portrayal of the situation rather than a straightforward news report. The use of quotes around \"REPORTER\" indicates a lack of seriousness or credibility regarding the individual being discussed. Additionally, the term \"WHINES\" implies a negative connotation, further suggesting that this news body is not presenting factual information but rather an opinion or commentary. Therefore, it does not meet the criteria for being a real news report.',\n",
      "    answer='fake'\n",
      "))]\n",
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 1 / 1  (100.0):   5%|â–         | 1/21 [00:01<00:33,  1.67s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 2 / 2  (100.0):   5%|â–         | 1/21 [00:01<00:33,  1.67s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 3  (100.0):  10%|â–‰         | 2/21 [00:01<00:31,  1.67s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 5 / 5  (100.0):  19%|â–ˆâ–‰        | 4/21 [00:02<00:08,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 6 / 6  (100.0):  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:02<00:05,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 7 / 7  (100.0):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:03<00:05,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 8 / 8  (100.0):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:03<00:05,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 10 / 10  (100.0):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:03<00:04,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 11 / 11  (100.0):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:04<00:03,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 12 / 12  (100.0):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:04<00:03,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 13 / 13  (100.0):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:05<00:03,  2.56it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 14 / 14  (100.0):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:05<00:02,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 15 / 15  (100.0):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:05<00:02,  3.16it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 15 / 15  (100.0):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:06<00:02,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 16  (100.0):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:06<00:01,  2.57it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18 / 18  (100.0):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:07<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 19 / 19  (100.0):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:07<00:00,  2.79it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Average Metric: 20 / 20  (100.0):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:07<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Looking at the traces, []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21 / 21  (100.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:07<00:00,  2.71it/s]\n",
      "INFO:dspy.evaluate.evaluate:\u001b[2m2024-08-17T12:38:27.949350Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 21 / 21 (100.0%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the traces, []\n",
      "Score: 100.0 for set: [4]\n",
      "New best sscore: 100.0 for seed 1\n",
      "Scores so far: [95.24, 85.71, 95.24, 80.95, 100.0]\n",
      "Best score: 100.0\n",
      "Average of max per entry across top 1 scores: 1.0\n",
      "Average of max per entry across top 2 scores: 1.0\n",
      "Average of max per entry across top 3 scores: 1.0\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n",
      "5 candidate programs found.\n"
     ]
    }
   ],
   "source": [
    "optimized_cotcomb = teleprompter.compile(model_that_solves_task,\n",
    "                                    trainset=custom_trainset,\n",
    "                                    valset=custom_devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ad76e-4f80-4f8e-9dea-3a70a3e9b32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c1936-983b-4908-9492-714377ecd1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c24052-88df-47f5-a8e4-1c3ae574d5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae3dc9-d7b5-4a5f-a876-d748ecacb536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7ec8c-6046-4b75-94c7-2a5bfb039c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590c2aa-f519-40ae-a7f6-08c47f3f9576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6daa12-0f8a-4ab3-ae16-0f1ea51c363e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c8032-ed71-4427-a8c4-173cc59f4817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddc5ba-c71f-4469-8f84-9234cd23ab86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
